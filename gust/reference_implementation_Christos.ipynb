{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gust  # library for loading graph data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import time\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using `gust` library\n",
    "# graph.standardize() makes the graph unweighted, undirected and selects\n",
    "# the largest connected component\n",
    "# graph.unpack() returns the necessary vectors / matrices\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from utils import graph_util\n",
    "A,  X = graph_util.load_dataset('cora')\n",
    "\n",
    "\n",
    "# A - adjacency matrix \n",
    "# X - attribute matrix - not needed\n",
    "# y - node labels\n",
    "\n",
    "if (A != A.T).sum() > 0:\n",
    "    raise RuntimeError(\"The graph must be undirected!\")\n",
    "\n",
    "if (A.data != 1).sum() > 0:\n",
    "    raise RuntimeError(\"The graph must be unweighted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2485, 2485])\n"
     ]
    }
   ],
   "source": [
    "num_nodes = A.shape[0]\n",
    "num_edges = A.sum()\n",
    "\n",
    "# Convert adjacency matrix to a CUDA Tensor\n",
    "adj = torch.FloatTensor(A.toarray()).cuda()\n",
    "\n",
    "adj.nonzero()\n",
    "print(adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "# Define the embedding matrix\n",
    "embedding_dim = 64\n",
    "emb = nn.Parameter(torch.empty(num_nodes, embedding_dim).normal_(0.0, 1.0))\n",
    "\n",
    "# Initialize the bias\n",
    "# The bias is initialized in such a way that if the dot product between two embedding vectors is 0 \n",
    "# (i.e. z_i^T z_j = 0), then their connection probability is sigmoid(b) equals to the \n",
    "# background edge probability in the graph. This significantly speeds up training\n",
    "edge_proba = num_edges / (num_nodes**2 - num_nodes)\n",
    "bias_init = np.log(edge_proba / (1 - edge_proba))\n",
    "b = nn.Parameter(torch.Tensor([bias_init]))\n",
    "\n",
    "# Regularize the embeddings but don't regularize the bias\n",
    "# The value of weight_decay has a significant effect on the performance of the model (don't set too high!)\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': [emb], 'weight_decay': 1e-7},\n",
    "    {'params': [b]}],\n",
    "    lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many ways to compute the loss / negative log-likelihood of the model\n",
    "def compute_loss_v1(adj, emb, b=0.0): \n",
    "    \"\"\"Compute the negative log-likelihood of the Bernoulli model.\"\"\"\n",
    "    logits = emb @ emb.t() + b\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, adj, reduction='none')\n",
    "    # Since we consider graphs without self-loops, we don't want to compute loss\n",
    "    # for the diagonal entries of the adjacency matrix.\n",
    "    # This will kill the gradients on the diagonal.\n",
    "    loss[np.diag_indices(adj.shape[0])] = 0.0\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "# Approach 1: Naive apporach\n",
    "def compute_loss_d1(adj, emb, b=0.0): \n",
    "    \"\"\"Compute the rdf distance of the Bernoulli model.\"\"\"\n",
    "    # Initialization\n",
    "    start_time = time.time()\n",
    "    N,d=emb.shape\n",
    "    squared_euclidian = torch.zeros(N,N).cuda()\n",
    "    gamma= 0.1\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for initialization = {duration:.5f}')\n",
    "    # Compute squared euclidian\n",
    "    start_time = time.time()\n",
    "    for index, embedding in enumerate(emb):\n",
    "        sub =  embedding-emb + 10e-9\n",
    "        squared_euclidian[index,:]= torch.sum(torch.pow(sub,2),1)\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for euclidian = {duration:.5f}')\n",
    "    # Compute exponentianl\n",
    "    start_time = time.time()\n",
    "    radial_exp = torch.exp (-gamma * torch.sqrt(squared_euclidian))\n",
    "    loss = F.binary_cross_entropy(radial_exp, adj, reduction='none')\n",
    "    loss[np.diag_indices(adj.shape[0])] = 0.0\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for loss  = {duration:.5f}')\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_degree_matrix(adj_np):\n",
    "    degree= torch.from_numpy(adj_np.sum(axis=1))\n",
    "    return degree\n",
    "    \n",
    "def compute_transition(adj_gpu,adj_np):\n",
    "    degree= compute_degree_matrix(adj_np)\n",
    "    inv_degree=torch.diagflat(1/degree).cuda()\n",
    "\n",
    "    P = inv_degree.mm(adj_gpu) \n",
    "    return P\n",
    "\n",
    "\n",
    "\n",
    "def compute_ppr(adj_gpu, adj_np, dim, alpha = 0.1 ):\n",
    "    term_2 = (1-alpha)*compute_transition(adj_gpu,adj_np)\n",
    "    print(term_2)\n",
    "    term_1 = torch.eye(dim,device='cuda')\n",
    "    matrix = term_1-term_2\n",
    "    print(matrix.shape)\n",
    "    P = alpha*torch.inverse(matrix)\n",
    "    return P\n",
    "\n",
    "\n",
    "def compute_loss_KL(adj_np, emb, method, b=0.0):\n",
    "    N,D=adj_np.shape\n",
    "    adj_gpu = torch.FloatTensor(adj_np.toarray()).cuda()\n",
    "    if method=='transition':\n",
    "        sim = compute_transition(adj_gpu,adj_np)\n",
    "    if method=='ppr':\n",
    "        sim = compute_ppr(adj_gpu,adj_np,N)\n",
    "    loss = -(sim*torch.log( 10e-9+ F.softmax(emb.mm(emb.t() ),dim=1,dtype=torch.float)))\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In general, it's very important to compute all the losses in a numerically stable way\n",
    "# (e.g. using the log-sum-exp trick) or use existing library functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5000\n",
    "display_step = 250\n",
    "\n",
    "\n",
    "compute_loss = compute_loss_KL\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss(A, emb, 'ppr', b)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Training loss is printed every display_step epochs\n",
    "    if epoch % display_step == 0:\n",
    "        print(f'Epoch {epoch:4d}, loss = {loss.item():.5f}')\n",
    "        \n",
    "        \n",
    "cora_KL = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to transform 64-dimensional embedding into 2d for visualization\n",
    "# For this we can either use t-SNE from scikit-learn or UMAP\n",
    "# umap package can be installed with `pip install umap`\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# from umap import UMAP as TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(emb, y):\n",
    "    emb = emb.cpu().detach().numpy()\n",
    "    tsne = TSNE()\n",
    "    vis = tsne.fit_transform(emb)\n",
    "    plt.figure(figsize=[10, 8])\n",
    "    plt.scatter(vis[:, 0], vis[:, 1], c=palette[y], s=20, alpha=0.8)\n",
    "    \n",
    "# Alternative to the default seaborn palette\n",
    "palette = np.array(sns.color_palette('muted', n_colors=len(np.unique(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize(emb, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_ratio = 0.2\n",
    "\n",
    "label=y\n",
    "\n",
    "h_datasets = ['cora sigmoid', 'cora KL'] \n",
    "\n",
    "datasets = [cora_sigmoid, cora_KL]\n",
    "\n",
    "h_classifiers = [\"Decision Tree\",\"Nearest Neighbors\", \"Linear SVM\"\n",
    "         ]\n",
    "\n",
    "classifiers = [ \n",
    "    DecisionTreeClassifier(),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\")]\n",
    "\n",
    "\n",
    "\n",
    "for i, data in enumerate(datasets):\n",
    "    features = data.cpu().detach().numpy()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(features, label, test_size= split_ratio)\n",
    "    for j, model in enumerate(classifiers):\n",
    "        model.fit(X_train, Y_train)\n",
    "        score = model.score(X_test, Y_test)\n",
    "        print ( h_datasets[i], h_classifiers[j])\n",
    "        print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree= compute_degree_matrix(A)\n",
    "inv_degree=torch.diagflat(1/degree).cuda()\n",
    "    \n",
    "P = inv_degree.mm(adj) \n",
    "print(inv_degree)\n",
    "print(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "\n",
    "if type(emb) is not np.ndarray: \n",
    "    emb = emb.cpu().detach().numpy()\n",
    "\n",
    "X, labels_true = emb, y\n",
    "n_cluster = len(set(labels_true))\n",
    "init = np.zeros((n_cluster,embedding_dim))\n",
    "for i in range(n_cluster):\n",
    "    init[i,:] = X[np.where(labels_true==i)].mean(axis=0)\n",
    "kmeans =  KMeans(n_clusters=n_cluster, random_state=0, init= init).fit(X)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "print(\"Mutual Information: %0.3f\"\n",
    "      % mutual_info_score(labels_true, labels))\n",
    "print(\"Normalized Mutual Information: %0.3f\"\n",
    "      % normalized_mutual_info_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % adjusted_mutual_info_score(labels_true, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "palette = np.array(sns.color_palette('muted', n_colors=len(np.unique(y))))\n",
    "tsne = TSNE()\n",
    "vis = tsne.fit_transform(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 8])\n",
    "plt.scatter(vis[:, 0], vis[:, 1], c=palette[labels], s=20, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 8])\n",
    "plt.scatter(vis[:, 0], vis[:, 1], c=palette[labels_true], s=20, alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models that need to be implemented\n",
    "## 1. Bernoulli models\n",
    "Learn embeddings by maximizing the objective \n",
    "$$\\max_{Z \\in \\mathbb{R}^{N \\times D}} \\log p(A | Z)$$\n",
    "where\n",
    "$$p(A | Z) = \\prod_{i < j} Bernoulli(A_{ij}| f(z_i, z_j))$$\n",
    "\n",
    "- Sigmoid model\n",
    "$$f(z_i, z_j) = \\sigma(z_i^T z_j + b)$$\n",
    "\n",
    "$\\;$\n",
    "- Distance-based model #1\n",
    "$$f(z_i, z_j) = \\exp(-\\gamma||z_i^T - z_j||)$$\n",
    "\n",
    "$\\;$\n",
    "- Distance-based model #2 (https://arxiv.org/pdf/1905.13177.pdf, Equation 6)\n",
    "$$f(z_i, z_j) = \\sigma(C(1 - ||z_i^T - z_j||))$$\n",
    "they use $C = 10$ in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Categorical cross-entropy models\n",
    "Learn embeddings by optimizing the objective \n",
    "$$\\min_{Z \\in \\mathbb{R}^{N \\times D}} \\mathbb{KL}(M || \\operatorname{softmax}(Z Z^T))$$\n",
    "note that we don't need to add a bias term here since $\\operatorname{softmax}(x) = \\operatorname{softmax}(x + c)$ for any vector $x$ and constant $c$.\n",
    "\n",
    "Choices for $M$:\n",
    "- Transition matrix $M = P = D^{-1}A$, where $D_{ii} = \\sum_{ij} A_{ij}$, $D_{ij} = 0$ if $i \\ne j$.\n",
    "- Personalized PageRank matrix $M = (I - \\alpha P)^{-1}$(https://arxiv.org/pdf/1803.04742.pdf)\n",
    "- Finite-step transition matrix (i.e. average of powers of the transition matrix) $\\frac{1}{T} \\sum_{t=1}^{T} P^{t}$ (https://arxiv.org/pdf/1702.05764.pdf). This is equivalent to the popular DeepWalk method (https://arxiv.org/abs/1403.6652)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different model variants for (1) and (2)\n",
    "\n",
    "You should consider two options for modeling the embeddings:\n",
    " - Learning $Z \\in \\mathbb{R}^{N \\times D}$, get a \"score\" as $z_i^T z_j$\n",
    " - Learning $Z \\in \\mathbb{R}^{N \\times D}$ and $W \\in \\mathbb{R}^{D \\times D}$, get a \"score\" as $z_i^T W z_j$\n",
    " \n",
    "The first option might not be capable to model networks with heterophily, but the second option requires learning more parameters. You should implement both version and see which works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods based on SVD / Matrix factorization\n",
    "You obtain embeddings in these methods by performing SVD / eigendecomposition (no need to perform gradient descent here).\n",
    "\n",
    "- NetMF - see Algorithm 3 & 4 in (https://arxiv.org/pdf/1710.02971.pdf)\n",
    "- Spectral clustering - see MMDS lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error for the second approach\n",
    "compute_loss = compute_loss_d2\n",
    "max_epochs = 5000\n",
    "display_step = 250\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss(adj, emb, b)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Training loss is printed every display_step epochs\n",
    "    if epoch % display_step == 0:\n",
    "        print(f'Epoch {epoch:4d}, loss = {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error for the third approach\n",
    "compute_loss = compute_loss_d3\n",
    "max_epochs = 5000\n",
    "display_step = 250\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss(adj, emb, b)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Training loss is printed every display_step epochs\n",
    "    if epoch % display_step == 0:\n",
    "        print(f'Epoch {epoch:4d}, loss = {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error for the fourth approach\n",
    "compute_loss = compute_loss_d4\n",
    "max_epochs = 5000\n",
    "display_step = 250\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss(adj, emb, b)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Training loss is printed every display_step epochs\n",
    "    if epoch % display_step == 0:\n",
    "        print(f'Epoch {epoch:4d}, loss = {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second apporach with the squareform function \n",
    "from scipy.spatial.distance import squareform\n",
    "start_time = time.time()\n",
    "N=1000\n",
    "emb= torch.randn(N, N).cuda()\n",
    "euclidian= torch.nn.functional.pdist(emb, p=2)\n",
    "euclidian_np= torch.from_numpy(euclidian.cpu().detach().numpy())\n",
    "matrix_euclidian_np = squareform(euclidian_np)\n",
    "matrix_euclidian = torch.from_numpy(matrix_euclidian_np).cuda()\n",
    "end_time= time.time()\n",
    "duration= end_time -start_time\n",
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The third apporach\n",
    "start_time = time.time()\n",
    "N=1000\n",
    "emb= torch.randn(N, N).cuda()\n",
    "euclidian= torch.nn.functional.pdist(emb, p=2)\n",
    "start= 0\n",
    "for i in range(N):\n",
    "    end = start + ( N - i -1)\n",
    "    elem = len(euclidian[start:end])\n",
    "    #print(f' start = {start:d}, end = {end:d}, elements = {elem:d} ')\n",
    "    matrix_euclidian[i,i+1:] = euclidian[start:end] \n",
    "    matrix_euclidian[i+1:,i] = euclidian[start:end] \n",
    "    start += (N-i-1) \n",
    "#print(matrix_euclidian)    \n",
    "end_time= time.time()\n",
    "end_time -start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_ratio = 0.2\n",
    "\n",
    "label=y\n",
    "\n",
    "\n",
    "n_iterations = 10\n",
    "n_models =2\n",
    "total_scores = []\n",
    "\n",
    "if type(emb) is not np.ndarray: \n",
    "    emb = emb.cpu().detach().numpy()\n",
    "for i in range(n_models):\n",
    "    score_model= np.zeros(n_iterations)\n",
    "    for i in range(n_iterations):\n",
    "        features = emb\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(features, label, test_size= split_ratio, random_state=i) \n",
    "        model = KNeighborsClassifier(3).fit(X_train, Y_train)\n",
    "        score = model.score(X_test, Y_test)\n",
    "        score_model[i]= score\n",
    "    total_scores.append(score_model)\n",
    "\n",
    "sns.boxplot(data=total_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
