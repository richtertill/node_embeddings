{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gust  # library for loading graph data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import time\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/students/winter-term-2019/project_5/cziakas/project-5/gust/gust/preprocessing.py:238: UserWarning: 124 self loops removed\n",
      "  warnings.warn(\"{0} self loops removed\".format(num_self_loops))\n"
     ]
    }
   ],
   "source": [
    "A_cora, X_cora, _, y_cora = gust.load_dataset('cora').standardize().unpack()\n",
    "A_cita, X_cita, _, y_cita = gust.load_dataset('citeseer').standardize().unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes_co = A_cora.shape[0]\n",
    "num_edges_co = A_cora.sum()\n",
    "\n",
    "num_nodes_ci = A_cita.shape[0]\n",
    "num_edges_ci = A_cita.sum()\n",
    "\n",
    "# Convert adjacency matrix to a CUDA Tensor\n",
    "adj_co = torch.FloatTensor(A_cora.toarray()).cuda()\n",
    "adj_ci = torch.FloatTensor(A_cita.toarray()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_edges' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0a6f31b09e5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# (i.e. z_i^T z_j = 0), then their connection probability is sigmoid(b) equals to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# background edge probability in the graph. This significantly speeds up training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0medge_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_edges\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_nodes_co\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnum_nodes_co\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mbias_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_proba\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0medge_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbias_init\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_edges' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "embedding_dim = 64\n",
    "emb_co = nn.Parameter(torch.empty(num_nodes_co, embedding_dim).normal_(0.0, 1.0))\n",
    "\n",
    "# Initialize the bias\n",
    "# The bias is initialized in such a way that if the dot product between two embedding vectors is 0 \n",
    "# (i.e. z_i^T z_j = 0), then their connection probability is sigmoid(b) equals to the \n",
    "# background edge probability in the graph. This significantly speeds up training\n",
    "edge_proba = num_edges / (num_nodes_co**2 - num_nodes_co)\n",
    "bias_init = np.log(edge_proba / (1 - edge_proba))\n",
    "b = nn.Parameter(torch.Tensor([bias_init]))\n",
    "\n",
    "# Regularize the embeddings but don't regularize the bias\n",
    "# The value of weight_decay has a significant effect on the performance of the model (don't set too high!)\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': [emb], 'weight_decay': 1e-7},\n",
    "    {'params': [b]}],\n",
    "    lr=1e-2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many ways to compute the loss / negative log-likelihood of the model\n",
    "def compute_loss_v1(adj, emb, b=0.0): \n",
    "    \"\"\"Compute the negative log-likelihood of the Bernoulli model.\"\"\"\n",
    "    logits = emb @ emb.t() + b\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, adj, reduction='none')\n",
    "    # Since we consider graphs without self-loops, we don't want to compute loss\n",
    "    # for the diagonal entries of the adjacency matrix.\n",
    "    # This will kill the gradients on the diagonal.\n",
    "    loss[np.diag_indices(adj.shape[0])] = 0.0\n",
    "    return loss.mean()\n",
    "\n",
    "# This function uses the torch.distributions module\n",
    "def compute_loss_v2(adj, emb, b=0.0): \n",
    "    \"\"\"Compute the negative log-likelihood of the Bernoulli model.\"\"\"\n",
    "    logits = emb @ emb.t() + b\n",
    "    distribution = dist.Bernoulli(logits=logits)\n",
    "    log_probas = distribution.log_prob(adj)\n",
    "    log_probas[np.diag_indices(adj.shape[0])] = 0.0\n",
    "    loss = -log_probas.mean()\n",
    "    return loss\n",
    "\n",
    "# Here we compute the loss manually\n",
    "def compute_loss_v3(adj, emb, b=0.0): \n",
    "    \"\"\"Compute the negative log-likelihood of the Bernoulli model.\"\"\"\n",
    "    logits = emb @ emb.t() + b\n",
    "    log_probas = adj * logits - F.softplus(logits)\n",
    "    log_probas[np.diag_indices(adj.shape[0])] = 0.0\n",
    "    loss = -log_probas.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Approach 1: Naive apporach\n",
    "def compute_loss_d1(adj, emb, b=0.0): \n",
    "    \"\"\"Compute the rdf distance of the Bernoulli model.\"\"\"\n",
    "    # Initialization\n",
    "    start_time = time.time()\n",
    "    N,d=emb.shape\n",
    "    squared_euclidian = torch.zeros(N,N).cuda()\n",
    "    gamma= 0.1\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for initialization = {duration:.5f}')\n",
    "    # Compute squared euclidian\n",
    "    start_time = time.time()\n",
    "    for index, embedding in enumerate(emb):\n",
    "        sub =  embedding-emb + 10e-9\n",
    "        squared_euclidian[index,:]= torch.sum(torch.pow(sub,2),1)\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for euclidian = {duration:.5f}')\n",
    "    # Compute exponentianl\n",
    "    start_time = time.time()\n",
    "    radial_exp = torch.exp (-gamma * torch.sqrt(squared_euclidian))\n",
    "    loss = F.binary_cross_entropy(radial_exp, adj, reduction='none')\n",
    "    loss[np.diag_indices(adj.shape[0])] = 0.0\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for loss  = {duration:.5f}')\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "# Approach 2: Use the squareform function\n",
    "def compute_loss_d2(adj, emb, b=0.0): \n",
    "    # Initialization\n",
    "    start_time = time.time()\n",
    "    N,d=emb.shape\n",
    "    squared_euclidian = torch.zeros(N,N).cuda()\n",
    "    gamma= 0.1\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for initialization = {duration:.5f}')\n",
    "    # Compute squared euclidian\n",
    "    start_time = time.time()\n",
    "    euclidian= torch.nn.functional.pdist(emb, p=2)\n",
    "    euclidian_np= torch.from_numpy(euclidian.cpu().detach().numpy())\n",
    "    matrix_euclidian_np = squareform(euclidian_np)\n",
    "    matrix_euclidian = torch.from_numpy(matrix_euclidian_np).cuda()\n",
    "    end_time= time.time()\n",
    "    #print(f' Time for euclidian = {duration:.5f}')\n",
    "    # Compute exponentianl\n",
    "    start_time = time.time()\n",
    "    radial_exp = torch.exp (-gamma * matrix_euclidian)\n",
    "    loss = F.binary_cross_entropy(radial_exp, adj, reduction='none')\n",
    "    loss[np.diag_indices(adj.shape[0])] = 0.0\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for loss  = {duration:.5f}')\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "# Approach 3: Build the matrix from scratch\n",
    "def compute_loss_d3(adj, emb, b=0.0): \n",
    "    # Initialization\n",
    "    start_time = time.time()\n",
    "    N,d=emb.shape\n",
    "    squared_euclidian = torch.zeros(N,N).cuda()\n",
    "    gamma= 0.1\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for initialization = {duration:.5f}')\n",
    "    # Compute squared euclidian\n",
    "    start_time = time.time()\n",
    "    euclidian= torch.nn.functional.pdist(emb, p=2)\n",
    "    start= 0\n",
    "    for i in range(N):\n",
    "        end = start + ( N - i -1)\n",
    "        elem = len(euclidian[start:end])\n",
    "        #print(f' start = {start:d}, end = {end:d},elements = {elem:d} ')\n",
    "        squared_euclidian[i,i+1:] = euclidian[start:end] \n",
    "        squared_euclidian[i+1:,i] = euclidian[start:end] \n",
    "        start += (N-i-1) \n",
    "        end_time= time.time()\n",
    "    #print(f' Time for euclidian = {duration:.5f}')\n",
    "    # Compute exponentianl\n",
    "    start_time = time.time()\n",
    "    radial_exp = torch.exp (-gamma * squared_euclidian)\n",
    "    loss = F.binary_cross_entropy(radial_exp, adj, reduction='none')\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for loss  = {duration:.5f}')\n",
    "    return loss.mean()\n",
    "\n",
    "# Approach 4: Hold only a part of A \n",
    "def compute_loss_d4(adj, emb, b=0.1): \n",
    "    # Initialization\n",
    "    N,d=emb.shape\n",
    "    gamma= 0.1\n",
    "    # Compute the rdf distance\n",
    "    euclidian= torch.nn.functional.pdist(emb, p=2)\n",
    "    radial_exp = torch.exp (-gamma * euclidian)\n",
    "    # Extract the elements of the upper triangular matrix without the diagonal elements\n",
    "    ind=torch.triu_indices(N,N,offset=1)\n",
    "    labels = adj[ind[0].cpu().detach().numpy(),ind[1].cpu().detach().numpy()]\n",
    "    # Compute the loss function\n",
    "    loss = F.binary_cross_entropy(radial_exp, labels, reduction='none')\n",
    "    return loss.mean()\n",
    "\n",
    "# Approach 4: Hold only a part of A \n",
    "def compute_loss_d5(adj, emb, b=0.1): \n",
    "    # Initialization\n",
    "    N,d=emb.shape\n",
    "    gamma= 0.1\n",
    "    # Compute the rdf distance\n",
    "    euclidian= torch.nn.functional.pdist(emb, p=2).cpu()\n",
    "    radial_exp = torch.exp (-gamma * euclidian).cpu()\n",
    "    # Extract the elements of the upper triangular matrix without the diagonal elements\n",
    "    ind=torch.triu_indices(N,N,offset=1).cpu()\n",
    "    labels = adj[ind[0],ind[1]]\n",
    "    # Compute the loss function\n",
    "    loss = F.binary_cross_entropy(radial_exp.cuda(), labels.cuda(), reduction='none')\n",
    "    return loss.mean()\n",
    "\n",
    "def compute_loss_KL(adj, emb, b=0.0):\n",
    "    degree= torch.from_numpy(A.sum(axis=1))\n",
    "    inv_degree=torch.diagflat(1/degree).cuda()\n",
    "    P = inv_degree.mm(adj) \n",
    "    loss = -(P*torch.log( 10e-9+ F.softmax(emb.mm(emb.t() ),dim=1,dtype=torch.float)))\n",
    "    return loss.mean()\n",
    "\n",
    "compute_loss = compute_loss_KL\n",
    "\n",
    "\n",
    "def compute_loss_ber_exp1(adj, emb, b=0.1):\n",
    "    #Init\n",
    "    N,d=emb.shape\n",
    "    gamma=0.001\n",
    "    \n",
    "    #get indices of upper triangular matrix\n",
    "    ind=torch.triu_indices(N,N,offset=1)\n",
    "    \n",
    "    #compute f(z_i,z_j) = exp(-gamma||z_i-z_j||^2)\n",
    "    dist=F.pdist(emb, p=2)\n",
    "    #put distances into upper triangular matrix\n",
    "    dist_matrix=torch.zeros(N,N,requires_grad=True)\n",
    "    dist_matrix[ind[0],ind[1]] = dist\n",
    "    \n",
    "    logits=torch.exp(-gamma * dist_matrix**2)\n",
    "    logits=logits[ind[0],ind[1]]\n",
    "\n",
    "    labels = adj[ind[0],ind[1]]\n",
    "    print('labels: ',labels,labels.size())\n",
    "    print('logits: ',logits,logits.size())\n",
    "    #compute loss\n",
    "    loss = F.binary_cross_entropy_with_logits(10e-9+ logits , labels, reduction='mean')\n",
    "\n",
    "\n",
    "# In general, it's very important to compute all the losses in a numerically stable way\n",
    "# (e.g. using the log-sum-exp trick) or use existing library functions\n",
    "\n",
    "\n",
    "def compute_loss_ber_exp1(adj, emb, b=0.1):\n",
    "    #Init\n",
    "    N,d=emb.shape\n",
    "    gamma=0.001\n",
    "    \n",
    "    #get indices of upper triangular matrix\n",
    "    ind=torch.triu_indices(N,N,offset=1)\n",
    "    \n",
    "    #compute f(z_i,z_j) = exp(-gamma||z_i-z_j||^2)\n",
    "    dist=F.pdist(emb, p=2)\n",
    "    #put distances into upper triangular matrix\n",
    "    dist_matrix=torch.zeros(N,N,requires_grad=True)\n",
    "    dist_matrix[ind[0],ind[1]] = dist\n",
    "    \n",
    "    logits=torch.exp(-gamma * dist_matrix**2)\n",
    "    logits=logits[ind[0],ind[1]]\n",
    "\n",
    "    labels = adj[ind[0],ind[1]]\n",
    "    print('labels: ',labels,labels.size())\n",
    "    print('logits: ',logits,logits.size())\n",
    "    #compute loss\n",
    "    loss = F.binary_cross_entropy_with_logits(10e-9+ logits , labels, reduction='mean')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
