{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gust  # library for loading graph data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import time\n",
    "from scipy.spatial.distance import squareform\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using `gust` library\n",
    "# graph.standardize() makes the graph unweighted, undirected and selects\n",
    "# the largest connected component\n",
    "# graph.unpack() returns the necessary vectors / matrices\n",
    "\n",
    "A, X, _, y = gust.load_dataset('cora').standardize().unpack()\n",
    "# A - adjacency matrix \n",
    "# X - attribute matrix - not needed\n",
    "# y - node labels\n",
    "\n",
    "if (A != A.T).sum() > 0:\n",
    "    raise RuntimeError(\"The graph must be undirected!\")\n",
    "\n",
    "if (A.data != 1).sum() > 0:\n",
    "    raise RuntimeError(\"The graph must be unweighted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = A.shape[0]\n",
    "num_edges = A.sum()\n",
    "\n",
    "# Convert adjacency matrix to a CUDA Tensor\n",
    "adj = torch.FloatTensor(A.toarray()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "# Define the embedding matrix\n",
    "embedding_dim = 64\n",
    "emb = nn.Parameter(torch.empty(num_nodes, embedding_dim).normal_(0.0, 1.0))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the bias\n",
    "# The bias is initialized in such a way that if the dot product between two embedding vectors is 0 \n",
    "# (i.e. z_i^T z_j = 0), then their connection probability is sigmoid(b) equals to the \n",
    "# background edge probability in the graph. This significantly speeds up training\n",
    "edge_proba = num_edges / (num_nodes**2 - num_nodes)\n",
    "bias_init = np.log(edge_proba / (1 - edge_proba))\n",
    "b = nn.Parameter(torch.Tensor([bias_init]))\n",
    "\n",
    "\n",
    "# Regularize the embeddings but don't regularize the bias\n",
    "# The value of weight_decay has a significant effect on the performance of the model (don't set too high!)\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': [emb], 'weight_decay': 1e-7}, {'params': [b]}],\n",
    "    lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_ber_sig(adj, emb, b=0.1): \n",
    "    #kernel: theta(z_i,z_j)=sigma(z_i^Tz_j+b)\n",
    "    # Initialization\n",
    "    N,d=emb.shape\n",
    "    \n",
    "    #compute f(z_i, z_j) = sigma(z_i^Tz_j+b)\n",
    "    dot=torch.matmul(emb,emb.T)\n",
    "    logits =dot+b\n",
    "    \n",
    "    #transform adj\n",
    "    ind=torch.triu_indices(N,N,offset=1)\n",
    "    logits = logits[ind[0], ind[1]] \n",
    "    labels = adj[ind[0],ind[1]]\n",
    "    \n",
    "    \n",
    "    #compute p(A|Z)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, labels, weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "def compute_loss_ber_exp1(adj, emb, b=0.1):\n",
    "    #Init\n",
    "    N,d=emb.shape\n",
    "    gamma=0.001\n",
    "    \n",
    "    print('adj', adj.size(), adj.dtype, type(adj), adj.requires_grad, adj.device)\n",
    "    print('emb', emb.size(), emb.dtype, type(emb), emb.requires_grad, emb.device)\n",
    "    \n",
    "    \n",
    "    #get indices of upper triangular matrix\n",
    "    ind=torch.triu_indices(N,N,offset=1)\n",
    "    labels = adj[ind[0],ind[1]]\n",
    "    \n",
    "    #compute f(z_i,z_j) = exp(-gamma||z_i-z_j||^2)\n",
    "    dist=F.pdist(emb, p=2)\n",
    "    print('dist', dist, dist.size(), dist.dtype, type(dist), dist.requires_grad, dist.device)\n",
    "    \n",
    "    #put distances into upper triangular matrix\n",
    "    #dist_matrix0 = nn.Parameter(torch.empty(num_nodes, embedding_dim))\n",
    "    \n",
    "    logits=torch.exp(-gamma * dist**2)\n",
    "    print('logits: ', logits.size(), logits.dtype, type(logits), logits.requires_grad, logits.device)\n",
    "\n",
    "    \n",
    "    print('labels', labels.size(), labels.dtype, type(labels), labels.requires_grad, labels.device)\n",
    "    #compute loss\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='mean')\n",
    "    print(loss)\n",
    "    return loss\n",
    "\n",
    "def compute_loss_d1(adj, emb, b=0.0): \n",
    "    \"\"\"Compute the rdf distance of the Bernoulli model.\"\"\"\n",
    "    # Initialization\n",
    "    start_time = time.time()\n",
    "    N,d=emb.shape\n",
    "    squared_euclidian = torch.zeros(N,N).cuda()\n",
    "    gamma= 0.1\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for initialization = {duration:.5f}')\n",
    "    # Compute squared euclidian\n",
    "    start_time = time.time()\n",
    "    for index, embedding in enumerate(emb):\n",
    "        sub =  embedding-emb + 10e-9\n",
    "        squared_euclidian[index,:]= torch.sum(torch.pow(sub,2),1)\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for euclidian = {duration:.5f}')\n",
    "    # Compute exponentianl\n",
    "    start_time = time.time()\n",
    "    radial_exp = torch.exp (-gamma * torch.sqrt(squared_euclidian))\n",
    "    loss = F.binary_cross_entropy(radial_exp, adj, reduction='none')\n",
    "    loss[np.diag_indices(adj.shape[0])] = 0.0\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for loss  = {duration:.5f}')\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def compute_loss_ber_exp2(adj, emb, b=0.1):\n",
    "    #Init\n",
    "    N,d=emb.shape\n",
    "\n",
    "    #get indices of upper triangular matrix\n",
    "    ind=torch.triu_indices(N,N,offset=1)\n",
    "    \n",
    "    #compute f(z_i, z_j) = sigma(z_i^Tz_j+b)\n",
    "    dot=torch.matmul(emb,emb.T)\n",
    "    print('dist: ', dot, dot.size(), type(dot))\n",
    "    logits=1-torch.exp(-dot)\n",
    "    logits=logits[ind[0],ind[1]]\n",
    "    labels = adj[ind[0],ind[1]]\n",
    "    print('logits: ', logits, logits.size(), type(logits))\n",
    "    \n",
    "    #compute loss\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_loss_KL(adj, emb, b=0.0):\n",
    "    #adj = torch.FloatTensor(A.toarray()).cuda()\n",
    "    degree= torch.from_numpy(adj.sum(axis=1))\n",
    "    inv_degree=torch.diagflat(1/degree).cuda()\n",
    "    P = inv_degree.mm(adj) \n",
    "    loss = -(P*torch.log( 10e-9+ F.softmax(emb.mm(emb.t() ),dim=1,dtype=torch.float)))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj torch.Size([2485, 2485]) torch.float32 <class 'torch.Tensor'> False cuda:0\n",
      "emb torch.Size([2485, 64]) torch.float32 <class 'torch.nn.parameter.Parameter'> True cuda:0\n",
      "dist tensor([24.3387, 22.5350, 26.6627,  ..., 24.8556, 27.0524, 21.2161],\n",
      "       grad_fn=<PdistBackward>) torch.Size([3086370]) torch.float32 <class 'torch.Tensor'> True cuda:0\n",
      "logits:  torch.Size([3086370]) torch.float32 <class 'torch.Tensor'> True cuda:0\n",
      "labels torch.Size([3086370]) torch.float32 <class 'torch.Tensor'> False cuda:0\n",
      "tensor(1.0108, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid configuration argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1f961abb1fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Training loss is printed every display_step epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/students/winter-term-2019/project_5/richter/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/students/winter-term-2019/project_5/richter/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid configuration argument"
     ]
    }
   ],
   "source": [
    "max_epochs = 1000\n",
    "display_step = 250\n",
    "compute_loss = compute_loss_ber_exp1\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss(adj, emb, b)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Training loss is printed every display_step epochs\n",
    "    if epoch == 0 or (epoch + 1) % display_step == 0:\n",
    "        print(f'Epoch {epoch+1:4d}, loss = {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-71-54a037b251c7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-71-54a037b251c7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    exp1:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "exp1:\n",
    "    dist:  tensor([[ 0.0000, 11.7170, 11.2746,  ..., 12.0473, 10.1984, 11.3381],\n",
    "        [ 0.0000,  0.0000, 12.3547,  ..., 11.8961, 11.2169, 12.2756],\n",
    "        [ 0.0000,  0.0000,  0.0000,  ...,  9.9336, 11.4303, 12.5992],\n",
    "        ...,\n",
    "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 12.0894, 12.9180],\n",
    "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, 10.4011],\n",
    "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
    "       grad_fn=<IndexPutBackward>) torch.Size([2485, 2485]) <class 'torch.Tensor'>\n",
    "exp2: \n",
    "    dist:  tensor([[ 6.8276e+01,  2.0167e+00,  3.5997e-02,  ..., -5.0973e+00,\n",
    "          1.1831e+01,  4.7927e+00],\n",
    "        [ 2.0167e+00,  7.3249e+01, -1.0391e+01,  ..., -8.7648e-01,\n",
    "          3.3881e+00, -3.9294e+00],\n",
    "        [ 3.5997e-02, -1.0391e+01,  5.9067e+01,  ...,  1.3639e+01,\n",
    "         -6.0809e+00, -1.5023e+01],\n",
    "        ...,\n",
    "        [-5.0973e+00, -8.7648e-01,  1.3639e+01,  ...,  6.6841e+01,\n",
    "         -9.9354e+00, -1.5082e+01],\n",
    "        [ 1.1831e+01,  3.3881e+00, -6.0809e+00,  ..., -9.9354e+00,\n",
    "          5.9443e+01,  1.0573e+01],\n",
    "        [ 4.7927e+00, -3.9294e+00, -1.5023e+01,  ..., -1.5082e+01,\n",
    "          1.0573e+01,  6.9868e+01]], grad_fn=<MmBackward>) torch.Size([2485, 2485]) <class 'torch.Tensor'>\n",
    "        \n",
    "        \n",
    "exp1: \n",
    "    logits:  tensor([0.8717, 0.8806, 0.8570,  ..., 0.8640, 0.8463, 0.8975],\n",
    "       grad_fn=<IndexBackward>) torch.Size([3086370]) <class 'torch.Tensor'>\n",
    "exp2: \n",
    "    logits:  tensor([ 8.7085e-01,  2.0776e-02, -5.2848e+05,  ..., -2.1115e+04,\n",
    "        -3.6931e+06,  9.9997e-01], grad_fn=<IndexBackward>) torch.Size([3086370]) <class 'torch.Tensor'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
