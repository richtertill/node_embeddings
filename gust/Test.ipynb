{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gust  # library for loading graph data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import time\n",
    "import random\n",
    "from scipy.spatial.distance import squareform\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using `gust` library\n",
    "# graph.standardize() makes the graph unweighted, undirected and selects\n",
    "# the largest connected component\n",
    "# graph.unpack() returns the necessary vectors / matrices\n",
    "\n",
    "A, X, _, y = gust.load_dataset('cora').standardize().unpack()\n",
    "# A - adjacency matrix \n",
    "# X - attribute matrix - not needed\n",
    "# y - node labels\n",
    "A=A[:10,:10]\n",
    "\n",
    "if (A != A.T).sum() > 0:\n",
    "    raise RuntimeError(\"The graph must be undirected!\")\n",
    "\n",
    "if (A.data != 1).sum() > 0:\n",
    "    raise RuntimeError(\"The graph must be unweighted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it stochastic\n",
    "adj = torch.FloatTensor(A.toarray()).cuda()\n",
    "'''\n",
    "from the paper Sampling from Large Graphs:\n",
    "We first choose node v uniformly at random. We then generate a random number x that is geometrically distributed\n",
    "with mean pf /(1 âˆ’ pf ). Node v selects x out-links incident\n",
    "to nodes that were not yet visited. Let w1, w2, . . . , wx denote the other ends of these selected links. We then apply\n",
    "this step recursively to each of w1, w2, . . . , wx until enough\n",
    "nodes have been burned. As the process continues, nodes\n",
    "cannot be visited a second time, preventing the construction\n",
    "from cycling. If the fire dies, then we restart it, i.e. select\n",
    "new node v uniformly at random. We call the parameter pf\n",
    "the forward burning probability.\n",
    "'''\n",
    "\n",
    "#1. choose first node v uniformly at random and store it\n",
    "v_new = np.random.randint(len(adj))\n",
    "nodes = torch.tensor([v_new])\n",
    "print('nodes: ', nodes)\n",
    "#2. generate random number x from geometrix distribution with mean pf/(1-pf)\n",
    "pf=0.3 #burning probability, evaluated as best from the given paper\n",
    "x = np.random.geometric(pf/(1-pf))\n",
    "\n",
    "#3. let idx choose x out-links\n",
    "w = (adj[v_new]==1).nonzero()\n",
    "if w.shape[0]>x:\n",
    "    idx_w = random.sample(range(0, w.shape[0]), x)    \n",
    "    w=w[idx_w]\n",
    "    \n",
    "#4. loop until 15% of the dataset is covered\n",
    "\n",
    "while len(nodes)<20:\n",
    "    v_new = w[0].item()\n",
    "    w = (adj[v_new]==1).nonzero()\n",
    "    for i in w:\n",
    "        for j in nodes:\n",
    "            if w[i]==nodes[j]:\n",
    "                w[i]=0\n",
    "    w = w.remove(0)\n",
    "    if w.shape[0]>x:\n",
    "        idx_w = random.sample(range(0, w.shape[0]), x)    \n",
    "    v_new = torch.tensor([v_new])\n",
    "    nodes = torch.cat((nodes,v_new),0)\n",
    "    print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_nodes = A.shape[0]\n",
    "num_edges = A.sum()\n",
    "\n",
    "# Convert adjacency matrix to a CUDA Tensor\n",
    "adj = torch.FloatTensor(A.toarray()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d81f4e9fe839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#get edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#torch.manual_seed(123)\n",
    "# Define the embedding matrix\n",
    "embedding_dim = 64\n",
    "emb = nn.Parameter(torch.empty(num_nodes, embedding_dim).normal_(0.0, 1.0))\n",
    "\n",
    "#get edges\n",
    "e1,e2 = adj.nonzero()\n",
    "\n",
    "\n",
    "# Initialize the bias\n",
    "# The bias is initialized in such a way that if the dot product between two embedding vectors is 0 \n",
    "# (i.e. z_i^T z_j = 0), then their connection probability is sigmoid(b) equals to the \n",
    "# background edge probability in the graph. This significantly speeds up training\n",
    "edge_proba = num_edges / (num_nodes**2 - num_nodes)\n",
    "bias_init = np.log(edge_proba / (1 - edge_proba))\n",
    "b = nn.Parameter(torch.Tensor([bias_init]))\n",
    "\n",
    "\n",
    "# Regularize the embeddings but don't regularize the bias\n",
    "# The value of weight_decay has a significant effect on the performance of the model (don't set too high!)\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': [emb], 'weight_decay': 1e-7}, {'params': [b]}],\n",
    "    lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_ber_sig(adj, emb, b=0.1): \n",
    "    #kernel: theta(z_i,z_j)=sigma(z_i^Tz_j+b)\n",
    "    # Initialization\n",
    "    N,d=emb.shape\n",
    "    \n",
    "    #compute f(z_i, z_j) = sigma(z_i^Tz_j+b)\n",
    "    dot=torch.matmul(emb,emb.T)\n",
    "    logits =dot+b\n",
    "    \n",
    "    #transform adj\n",
    "    ind=torch.triu_indices(N,N,offset=1)\n",
    "    logits = logits[ind[0], ind[1]] \n",
    "    labels = adj[ind[0],ind[1]]\n",
    "    \n",
    "    \n",
    "    #compute p(A|Z)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, labels, weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "def compute_loss_d1(adj, emb, b=0.0): \n",
    "    \"\"\"Compute the rdf distance of the Bernoulli model.\"\"\"\n",
    "    # Initialization\n",
    "    start_time = time.time()\n",
    "    N,d=emb.shape\n",
    "    squared_euclidian = torch.zeros(N,N).cuda()\n",
    "    gamma= 0.1\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for initialization = {duration:.5f}')\n",
    "    # Compute squared euclidian\n",
    "    start_time = time.time()\n",
    "    for index, embedding in enumerate(emb):\n",
    "        sub =  embedding-emb + 10e-9\n",
    "        squared_euclidian[index,:]= torch.sum(torch.pow(sub,2),1)\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for euclidian = {duration:.5f}')\n",
    "    # Compute exponentianl\n",
    "    start_time = time.time()\n",
    "    radial_exp = torch.exp (-gamma * torch.sqrt(squared_euclidian))\n",
    "    loss = F.binary_cross_entropy(radial_exp, adj, reduction='none')\n",
    "    loss[np.diag_indices(adj.shape[0])] = 0.0\n",
    "    end_time= time.time()\n",
    "    duration= end_time -start_time\n",
    "    #print(f' Time for loss  = {duration:.5f}')\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def compute_loss_ber_exp2(adj, emb, b=0.1):\n",
    "    #Init\n",
    "    N,d=emb.shape\n",
    "\n",
    "    #get indices of upper triangular matrix\n",
    "    ind=torch.triu_indices(N,N,offset=1)\n",
    "    \n",
    "    #compute f(z_i, z_j) = sigma(z_i^Tz_j+b)\n",
    "    dot=torch.matmul(emb,emb.T)\n",
    "    print('dist: ', dot, dot.size(), type(dot))\n",
    "    logits=1-torch.exp(-dot)\n",
    "    logits=logits[ind[0],ind[1]]\n",
    "    labels = adj[ind[0],ind[1]]\n",
    "    print('logits: ', logits, logits.size(), type(logits))\n",
    "    \n",
    "    #compute loss\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_loss_KL(adj, emb, b=0.0):\n",
    "    #adj = torch.FloatTensor(A.toarray()).cuda()\n",
    "    degree= torch.from_numpy(adj.sum(axis=1))\n",
    "    inv_degree=torch.diagflat(1/degree).cuda()\n",
    "    P = inv_degree.mm(adj) \n",
    "    loss = -(P*torch.log( 10e-9+ F.softmax(emb.mm(emb.t() ),dim=1,dtype=torch.float)))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_ber_exp1(adj, emb, b=0.1):\n",
    "    #Init\n",
    "    N,d=emb.shape\n",
    "    gamma=0.001\n",
    "    \n",
    "    print('adj', adj.size(), adj.dtype, type(adj), adj.requires_grad, adj.device)\n",
    "    print('emb', emb.size(), emb.dtype, type(emb), emb.requires_grad, emb.device)\n",
    "    \n",
    "    \n",
    "    #get indices of upper triangular matrix\n",
    "    ind=torch.triu_indices(N,N,offset=1)\n",
    "    labels = adj[ind[0],ind[1]]\n",
    "    \n",
    "    #compute f(z_i,z_j) = exp(-gamma||z_i-z_j||^2)\n",
    "    dist=F.pdist(emb, p=2)\n",
    "    print('dist', dist, dist.size(), dist.dtype, type(dist), dist.requires_grad, dist.device)\n",
    "    \n",
    "    #put distances into upper triangular matrix\n",
    "    #dist_matrix0 = nn.Parameter(torch.empty(num_nodes, embedding_dim))\n",
    "    \n",
    "    logits=torch.exp(-gamma * dist**2)\n",
    "    print('logits: ', logits.size(), logits.dtype, type(logits), logits.requires_grad, logits.device)\n",
    "\n",
    "    \n",
    "    print('labels', labels.size(), labels.dtype, type(labels), labels.requires_grad, labels.device)\n",
    "    #compute loss\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, labels, reduction='mean')\n",
    "    print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_ber_dist(adj,emb,b=0.1, eps = 1e-5):\n",
    "    pdist = ((Z[:, None] - Z[None, :]).por(2.0).sum(-1)+eps).sqrt()\n",
    "    neg_term = torch.log(-torch.expm1(-pdist)+eps)\n",
    "    ned_term[np.diag_indisces(N)] = 0.0\n",
    "    pos_term = -pdist[e1,e2]\n",
    "    ned_term[e1,e2] = 0.0\n",
    "    return -(pos_term.sum() + neg_term.sum()) / Z.shape[0]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_epochs = 1000\n",
    "display_step = 250\n",
    "compute_loss = compute_loss_ber_dist\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss(adj, emb, b)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Training loss is printed every display_step epochs\n",
    "    if epoch == 0 or (epoch + 1) % display_step == 0:\n",
    "        print(f'Epoch {epoch+1:4d}, loss = {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
