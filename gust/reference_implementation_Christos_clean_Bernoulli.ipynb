{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gust  # library for loading graph data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import time\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using `gust` library\n",
    "# graph.standardize() makes the graph unweighted, undirected and selects\n",
    "# the largest connected component\n",
    "# graph.unpack() returns the necessary vectors / matrices\n",
    "\n",
    "A, X, _, y = gust.load_dataset('cora').standardize().unpack()\n",
    "# A - adjacency matrix \n",
    "# X - attribute matrix - not needed\n",
    "# y - node labels\n",
    "\n",
    "if (A != A.T).sum() > 0:\n",
    "    raise RuntimeError(\"The graph must be undirected!\")\n",
    "\n",
    "if (A.data != 1).sum() > 0:\n",
    "    raise RuntimeError(\"The graph must be unweighted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_nodes = A.shape[0]\n",
    "num_edges = A.sum()\n",
    "\n",
    "# Convert adjacency matrix to a CUDA Tensor\n",
    "adj = torch.FloatTensor(A.toarray()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "# Define the embedding matrix\n",
    "embedding_dim = 64\n",
    "emb = nn.Parameter(torch.empty(num_nodes, embedding_dim).normal_(0.0, 1.0))\n",
    "\n",
    "# Initialize the bias\n",
    "# The bias is initialized in such a way that if the dot product between two embedding vectors is 0 \n",
    "# (i.e. z_i^T z_j = 0), then their connection probability is sigmoid(b) equals to the \n",
    "# background edge probability in the graph. This significantly speeds up training\n",
    "edge_proba = num_edges / (num_nodes**2 - num_nodes)\n",
    "bias_init = np.log(edge_proba / (1 - edge_proba))\n",
    "b = nn.Parameter(torch.Tensor([bias_init]))\n",
    "\n",
    "# Regularize the embeddings but don't regularize the bias\n",
    "# The value of weight_decay has a significant effect on the performance of the model (don't set too high!)\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': [emb], 'weight_decay': 1e-7},\n",
    "    {'params': [b]}],\n",
    "    lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many ways to compute the loss / negative log-likelihood of the model\n",
    "# There are many ways to compute the loss / negative log-likelihood of the model\n",
    "def compute_loss_sigmoid(adj, emb, b=0.0): \n",
    "    \"\"\"Compute the negative log-likelihood of the Bernoulli model.\"\"\"\n",
    "    logits = emb @ emb.t() + b\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, adj, reduction='none')\n",
    "    # Since we consider graphs without self-loops, we don't want to compute loss\n",
    "    # for the diagonal entries of the adjacency matrix.\n",
    "    # This will kill the gradients on the diagonal.\n",
    "    loss[np.diag_indices(adj.shape[0])] = 0.0\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss_gaussian(adj, emb, b=0.0):\n",
    "    eps=1e-5\n",
    "    N = adj.shape[0]\n",
    "    d=64\n",
    "    e1, e2 = adj.nonzero()\n",
    "    pdist = ((emb[:, None] - emb[None, :]).pow(2.0).sum(-1) + eps).sqrt()\n",
    "    neg_term = torch.log(-torch.expm1(-pdist) + 1e-5)\n",
    "    neg_term[np.diag_indices(N)] = 0.0\n",
    "    pos_term = -pdist[e1, e2]\n",
    "    neg_term[e1, e2] = 0.0\n",
    "    return -(pos_term.sum() + neg_term.sum()) / emb.shape[0]**2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In general, it's very important to compute all the losses in a numerically stable way\n",
    "# (e.g. using the log-sum-exp trick) or use existing library functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, loss = 1.01289\n",
      "Epoch  250, loss = 0.01116\n",
      "Epoch  500, loss = 0.00455\n",
      "Epoch  750, loss = 0.00280\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1000\n",
    "display_step = 250\n",
    "\n",
    "\n",
    "compute_loss = compute_loss_sigmoid\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss(adj, emb, b)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Training loss is printed every display_step epochs\n",
    "    if epoch % display_step == 0:\n",
    "        print(f'Epoch {epoch:4d}, loss = {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, loss = 0.01374\n",
      "Epoch  250, loss = 0.00745\n",
      "Epoch  500, loss = 0.00675\n",
      "Epoch  750, loss = 0.00619\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1000\n",
    "display_step = 250\n",
    "\n",
    "\n",
    "compute_loss =  compute_loss_gaussian\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss(A, emb, b)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Training loss is printed every display_step epochs\n",
    "    if epoch % display_step == 0:\n",
    "        print(f'Epoch {epoch:4d}, loss = {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information: 0.707\n",
      "Normalized Mutual Information: 0.379\n",
      "Adjusted Mutual Information: 0.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/students/winter-term-2019/project_5/cziakas/miniconda3/lib/python3.7/site-packages/sklearn/cluster/k_means_.py:972: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n",
      "/nfs/students/winter-term-2019/project_5/cziakas/miniconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/nfs/students/winter-term-2019/project_5/cziakas/miniconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:746: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "\n",
    "if type(emb) is not np.ndarray: \n",
    "    emb = emb.cpu().detach().numpy()\n",
    "\n",
    "X, labels_true = emb, y\n",
    "n_cluster = len(set(labels_true))\n",
    "init = np.zeros((n_cluster,embedding_dim))\n",
    "for i in range(n_cluster):\n",
    "    init[i,:] = X[np.where(labels_true==i)].mean(axis=0)\n",
    "kmeans =  KMeans(n_clusters=n_cluster, random_state=0, init= init).fit(X)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "print(\"Mutual Information: %0.3f\"\n",
    "      % mutual_info_score(labels_true, labels))\n",
    "print(\"Normalized Mutual Information: %0.3f\"\n",
    "      % normalized_mutual_info_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % adjusted_mutual_info_score(labels_true, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
