{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import gust  # library for loading graph data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import time\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using `gust` library\n",
    "# graph.standardize() makes the graph unweighted, undirected and selects\n",
    "# the largest connected component\n",
    "# graph.unpack() returns the necessary vectors / matrices\n",
    "\n",
    "A, X, _, y = gust.load_dataset('cora').standardize().unpack()\n",
    "# A - adjacency matrix \n",
    "# X - attribute matrix - not needed\n",
    "# y - node labels\n",
    "\n",
    "if (A != A.T).sum() > 0:\n",
    "    raise RuntimeError(\"The graph must be undirected!\")\n",
    "\n",
    "if (A.data != 1).sum() > 0:\n",
    "    raise RuntimeError(\"The graph must be unweighted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_nodes = A.shape[0]\n",
    "num_edges = A.sum()\n",
    "\n",
    "# Convert adjacency matrix to a CUDA Tensor\n",
    "adj = torch.FloatTensor(A.toarray()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "# Define the embedding matrix\n",
    "embedding_dim = 64\n",
    "emb = nn.Parameter(torch.empty(num_nodes, embedding_dim).normal_(0.0, 1.0))\n",
    "\n",
    "# Initialize the bias\n",
    "# The bias is initialized in such a way that if the dot product between two embedding vectors is 0 \n",
    "# (i.e. z_i^T z_j = 0), then their connection probability is sigmoid(b) equals to the \n",
    "# background edge probability in the graph. This significantly speeds up training\n",
    "edge_proba = num_edges / (num_nodes**2 - num_nodes)\n",
    "bias_init = np.log(edge_proba / (1 - edge_proba))\n",
    "b = nn.Parameter(torch.Tensor([bias_init]))\n",
    "\n",
    "# Regularize the embeddings but don't regularize the bias\n",
    "# The value of weight_decay has a significant effect on the performance of the model (don't set too high!)\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': [emb], 'weight_decay': 1e-7},\n",
    "    {'params': [b]}],\n",
    "    lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many ways to compute the loss / negative log-likelihood of the model\n",
    "# There are many ways to compute the loss / negative log-likelihood of the model\n",
    "def compute_loss_v1(adj, emb, b=0.0): \n",
    "    \"\"\"Compute the negative log-likelihood of the Bernoulli model.\"\"\"\n",
    "    logits = emb @ emb.t() + b\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, adj, reduction='none')\n",
    "    # Since we consider graphs without self-loops, we don't want to compute loss\n",
    "    # for the diagonal entries of the adjacency matrix.\n",
    "    # This will kill the gradients on the diagonal.\n",
    "    loss[np.diag_indices(adj.shape[0])] = 0.0\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss_gaussian(adj, emb, b=0.0):\n",
    "    eps=1e-5\n",
    "    N = adj.shape[0]\n",
    "    d=64\n",
    "    e1, e2 = adj.nonzero()\n",
    "    pdist = ((emb[:, None] - emb[None, :]).pow(2.0).sum(-1) + eps).sqrt()\n",
    "    neg_term = torch.log(-torch.expm1(-pdist) + 1e-5)\n",
    "    neg_term[np.diag_indices(N)] = 0.0\n",
    "    pos_term = -pdist[e1, e2]\n",
    "    neg_term[e1, e2] = 0.0\n",
    "    return -(pos_term.sum() + neg_term.sum()) / emb.shape[0]**2\n",
    "\n",
    "\n",
    "\n",
    "def distance2(adj, emb, b=0.0):\n",
    "    eps=1e-5\n",
    "    N = adj.shape[0]\n",
    "    d=64\n",
    "    e1, e2 = adj.nonzero()\n",
    "    pdist = ((emb[:, None] - emb[None, :]).pow(2.0).sum(-1) + eps).sqrt()\n",
    "    sigdist = 1/(1+torch.exp(-10*(1-pdist))+eps)\n",
    "    neg_term = torch.log(1-sigdist +eps)\n",
    "    neg_term[np.diag_indices(N)] = 0.0\n",
    "    pos_term = torch.log(sigdist+ eps)[e1, e2]\n",
    "    neg_term[e1,e2] = 0.0\n",
    "    return -(pos_term.sum() + neg_term.sum()) / emb.shape[0]**2\n",
    "\n",
    "# In general, it's very important to compute all the losses in a numerically stable way\n",
    "# (e.g. using the log-sum-exp trick) or use existing library functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, loss = 1.01289\n",
      "Epoch  250, loss = 0.01116\n",
      "Epoch  500, loss = 0.00455\n",
      "Epoch  750, loss = 0.00280\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1000\n",
    "display_step = 250\n",
    "\n",
    "\n",
    "compute_loss = compute_loss_v1\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss(adj, emb, b)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Training loss is printed every display_step epochs\n",
    "    if epoch % display_step == 0:\n",
    "        print(f'Epoch {epoch:4d}, loss = {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, loss = 0.01374\n",
      "Epoch  250, loss = 0.00745\n",
      "Epoch  500, loss = 0.00675\n",
      "Epoch  750, loss = 0.00619\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1000\n",
    "display_step = 250\n",
    "\n",
    "\n",
    "compute_loss =  compute_loss_gaussian\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss(A, emb, b)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Training loss is printed every display_step epochs\n",
    "    if epoch % display_step == 0:\n",
    "        print(f'Epoch {epoch:4d}, loss = {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, loss = 0.01888\n",
      "Epoch    1, loss = nan\n",
      "Epoch    2, loss = nan\n",
      "Epoch    3, loss = nan\n",
      "Epoch    4, loss = nan\n",
      "Epoch    5, loss = nan\n",
      "Epoch    6, loss = nan\n",
      "Epoch    7, loss = nan\n",
      "Epoch    8, loss = nan\n",
      "Epoch    9, loss = nan\n",
      "Epoch   10, loss = nan\n",
      "Epoch   11, loss = nan\n",
      "Epoch   12, loss = nan\n",
      "Epoch   13, loss = nan\n",
      "Epoch   14, loss = nan\n",
      "Epoch   15, loss = nan\n",
      "Epoch   16, loss = nan\n",
      "Epoch   17, loss = nan\n",
      "Epoch   18, loss = nan\n",
      "Epoch   19, loss = nan\n",
      "Epoch   20, loss = nan\n",
      "Epoch   21, loss = nan\n",
      "Epoch   22, loss = nan\n",
      "Epoch   23, loss = nan\n",
      "Epoch   24, loss = nan\n",
      "Epoch   25, loss = nan\n",
      "Epoch   26, loss = nan\n",
      "Epoch   27, loss = nan\n",
      "Epoch   28, loss = nan\n",
      "Epoch   29, loss = nan\n",
      "Epoch   30, loss = nan\n",
      "Epoch   31, loss = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8c96bd46af9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-99e476208e74>\u001b[0m in \u001b[0;36mdistance2\u001b[0;34m(adj, emb, b)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msigdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mneg_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msigdist\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mneg_term\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mpos_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigdist\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mneg_term\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_epochs = 1000\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "compute_loss = distance2\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.zero_grad()\n",
    "    loss = compute_loss(A, emb, b)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # Training loss is printed every display_step epochs\n",
    "    if epoch % display_step == 0:\n",
    "        print(f'Epoch {epoch:4d}, loss = {loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "\n",
    "if type(emb) is not np.ndarray: \n",
    "    emb = emb.cpu().detach().numpy()\n",
    "\n",
    "X, labels_true = emb, y\n",
    "n_cluster = len(set(labels_true))\n",
    "init = np.zeros((n_cluster,embedding_dim))\n",
    "for i in range(n_cluster):\n",
    "    init[i,:] = X[np.where(labels_true==i)].mean(axis=0)\n",
    "kmeans =  KMeans(n_clusters=n_cluster, random_state=0, init= init).fit(X)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "print(\"Mutual Information: %0.3f\"\n",
    "      % mutual_info_score(labels_true, labels))\n",
    "print(\"Normalized Mutual Information: %0.3f\"\n",
    "      % normalized_mutual_info_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % adjusted_mutual_info_score(labels_true, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
