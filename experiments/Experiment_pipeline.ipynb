{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# import embedding methods\n",
    "from embedding.bernoulli import Bernoulli\n",
    "from embedding.kl import KL\n",
    "from embedding.matrix_factorization import MatrixFactorization\n",
    "\n",
    "# import evaluation methods\n",
    "from evaluation import evaluate_link_prediction\n",
    "from evaluation import evaluate_node_classification\n",
    "from evaluation import evaluate_node_clustering\n",
    "# \n",
    "# import utils\n",
    "from utils import graph_util\n",
    "from utils import plot_util\n",
    "from utils import model_util\n",
    "\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of the current experiment are stored at experiments/results/2020_01_18_21_38\n",
      "Bernoulli_sigmoid_sigmoid_64_False\n",
      "\n",
      "Link prediction evaluation has started...\n",
      "\n",
      "\n",
      "Round: 1\n",
      "\n",
      "Epoch    0, loss = 0.98493\n",
      "\n",
      "Round: 2\n",
      "\n",
      "Epoch    0, loss = 0.98581\n",
      "\n",
      "Round: 3\n",
      "\n",
      "Epoch    0, loss = 0.97058\n",
      "\n",
      "Round: 4\n",
      "\n",
      "Epoch    0, loss = 0.97763\n",
      "\n",
      "Round: 5\n",
      "\n",
      "Epoch    0, loss = 0.99128\n",
      "\n",
      "Round: 6\n",
      "\n",
      "Epoch    0, loss = 0.97413\n",
      "\n",
      "Round: 7\n",
      "\n",
      "Epoch    0, loss = 0.98027\n",
      "\n",
      "Round: 8\n",
      "\n",
      "Epoch    0, loss = 0.97227\n",
      "\n",
      "Round: 9\n",
      "\n",
      "Epoch    0, loss = 0.97990\n",
      "\n",
      "Round: 10\n",
      "\n",
      "Epoch    0, loss = 0.96352\n",
      "\n",
      "=> mean auc score: 0.5720428789841625\n",
      "\n",
      "Node classification evaluation has started...\n",
      "\n",
      "Epoch    0, loss = 1.02647\n",
      "Epoch   25, loss = 0.34604\n",
      "Epoch   50, loss = 0.11754\n",
      "Epoch   75, loss = 0.05486\n",
      "Epoch  100, loss = 0.03418\n",
      "Epoch  125, loss = 0.02509\n",
      "Epoch  150, loss = 0.02010\n",
      "Epoch  175, loss = 0.01689\n",
      "Epoch  200, loss = 0.01459\n",
      "Epoch  225, loss = 0.01280\n",
      "Epoch  250, loss = 0.01135\n",
      "Epoch  275, loss = 0.01013\n",
      "Epoch  300, loss = 0.00910\n",
      "Epoch  325, loss = 0.00822\n",
      "Epoch  350, loss = 0.00747\n",
      "Epoch  375, loss = 0.00681\n",
      "Epoch  400, loss = 0.00625\n",
      "Epoch  425, loss = 0.00576\n",
      "Epoch  450, loss = 0.00533\n",
      "Epoch  475, loss = 0.00496\n",
      "Epoch  500, loss = 0.00464\n",
      "Epoch  525, loss = 0.00435\n",
      "Epoch  550, loss = 0.00410\n",
      "Epoch  575, loss = 0.00387\n",
      "Epoch  600, loss = 0.00367\n",
      "Epoch  625, loss = 0.00349\n",
      "Epoch  650, loss = 0.00333\n",
      "Epoch  675, loss = 0.00319\n",
      "Epoch  700, loss = 0.00306\n",
      "Epoch  725, loss = 0.00294\n",
      "Epoch  750, loss = 0.00283\n",
      "Epoch  775, loss = 0.00273\n",
      "Epoch  800, loss = 0.00264\n",
      "Epoch  825, loss = 0.00255\n",
      "Epoch  850, loss = 0.00247\n",
      "Epoch  875, loss = 0.00240\n",
      "Epoch  900, loss = 0.00233\n",
      "Epoch  925, loss = 0.00227\n",
      "Epoch  950, loss = 0.00221\n",
      "Epoch  975, loss = 0.00215\n",
      "Epoch 1000, loss = 0.00210\n",
      "Epoch 1025, loss = 0.00205\n",
      "Epoch 1050, loss = 0.00200\n",
      "Epoch 1075, loss = 0.00196\n",
      "Epoch 1100, loss = 0.00191\n",
      "Epoch 1125, loss = 0.00187\n",
      "Epoch 1150, loss = 0.00184\n",
      "Epoch 1175, loss = 0.00180\n",
      "Epoch 1200, loss = 0.00177\n",
      "Epoch 1225, loss = 0.00173\n",
      "Epoch 1250, loss = 0.00170\n",
      "Epoch 1275, loss = 0.00167\n",
      "Epoch 1300, loss = 0.00165\n",
      "Epoch 1325, loss = 0.00162\n",
      "Epoch 1350, loss = 0.00159\n",
      "Epoch 1375, loss = 0.00157\n",
      "Epoch 1400, loss = 0.00154\n",
      "Epoch 1425, loss = 0.00152\n",
      "Epoch 1450, loss = 0.00150\n",
      "Epoch 1475, loss = 0.00148\n",
      "Epoch 1500, loss = 0.00146\n",
      "Epoch 1525, loss = 0.00144\n",
      "Epoch 1550, loss = 0.00142\n",
      "Epoch 1575, loss = 0.00140\n",
      "Epoch 1600, loss = 0.00138\n",
      "Epoch 1625, loss = 0.00136\n",
      "Epoch 1650, loss = 0.00135\n",
      "Epoch 1675, loss = 0.00133\n",
      "Epoch 1700, loss = 0.00132\n",
      "Epoch 1725, loss = 0.00130\n",
      "Epoch 1750, loss = 0.00129\n",
      "Epoch 1775, loss = 0.00127\n",
      "Epoch 1800, loss = 0.00126\n",
      "Epoch 1825, loss = 0.00124\n",
      "Epoch 1850, loss = 0.00123\n",
      "Epoch 1875, loss = 0.00122\n",
      "Epoch 1900, loss = 0.00121\n",
      "Epoch 1925, loss = 0.00120\n",
      "Epoch 1950, loss = 0.00118\n",
      "Epoch 1975, loss = 0.00117\n",
      "Epoch 2000, loss = 0.00116\n",
      "Epoch 2025, loss = 0.00115\n",
      "Epoch 2050, loss = 0.00114\n",
      "Epoch 2075, loss = 0.00113\n",
      "Epoch 2100, loss = 0.00112\n",
      "Epoch 2125, loss = 0.00111\n",
      "Epoch 2150, loss = 0.00110\n",
      "Epoch 2175, loss = 0.00109\n",
      "Epoch 2200, loss = 0.00108\n",
      "Epoch 2225, loss = 0.00107\n",
      "Epoch 2250, loss = 0.00106\n",
      "Epoch 2275, loss = 0.00106\n",
      "Epoch 2300, loss = 0.00105\n",
      "Epoch 2325, loss = 0.00104\n",
      "Epoch 2350, loss = 0.00103\n",
      "Epoch 2375, loss = 0.00102\n",
      "Epoch 2400, loss = 0.00102\n",
      "Epoch 2425, loss = 0.00101\n",
      "Epoch 2450, loss = 0.00100\n",
      "Epoch 2475, loss = 0.00099\n",
      "Epoch 2500, loss = 0.00099\n",
      "Epoch 2525, loss = 0.00098\n",
      "Epoch 2550, loss = 0.00097\n",
      "Epoch 2575, loss = 0.00097\n",
      "Epoch 2600, loss = 0.00096\n",
      "Epoch 2625, loss = 0.00095\n",
      "Epoch 2650, loss = 0.00095\n",
      "Epoch 2675, loss = 0.00094\n",
      "Epoch 2700, loss = 0.00093\n",
      "Epoch 2725, loss = 0.00093\n",
      "Epoch 2750, loss = 0.00092\n",
      "Epoch 2775, loss = 0.00092\n",
      "Epoch 2800, loss = 0.00091\n",
      "Epoch 2825, loss = 0.00090\n",
      "Epoch 2850, loss = 0.00090\n",
      "Epoch 2875, loss = 0.00089\n",
      "Epoch 2900, loss = 0.00089\n",
      "Epoch 2925, loss = 0.00088\n",
      "Epoch 2950, loss = 0.00088\n",
      "Epoch 2975, loss = 0.00087\n",
      "Epoch 3000, loss = 0.00087\n",
      "Epoch 3025, loss = 0.00086\n",
      "Epoch 3050, loss = 0.00085\n",
      "Epoch 3075, loss = 0.00085\n",
      "Epoch 3100, loss = 0.00084\n",
      "Epoch 3125, loss = 0.00084\n",
      "Epoch 3150, loss = 0.00083\n",
      "Epoch 3175, loss = 0.00083\n",
      "Epoch 3200, loss = 0.00083\n",
      "Epoch 3225, loss = 0.00082\n",
      "Epoch 3250, loss = 0.00082\n",
      "Epoch 3275, loss = 0.00081\n",
      "Epoch 3300, loss = 0.00081\n",
      "Epoch 3325, loss = 0.00080\n",
      "Epoch 3350, loss = 0.00080\n",
      "Epoch 3375, loss = 0.00079\n",
      "Epoch 3400, loss = 0.00079\n",
      "Epoch 3425, loss = 0.00078\n",
      "Epoch 3450, loss = 0.00078\n",
      "Epoch 3475, loss = 0.00078\n",
      "Epoch 3500, loss = 0.00077\n",
      "Epoch 3525, loss = 0.00077\n",
      "Epoch 3550, loss = 0.00076\n",
      "Epoch 3575, loss = 0.00076\n",
      "Epoch 3600, loss = 0.00076\n",
      "Epoch 3625, loss = 0.00075\n",
      "Epoch 3650, loss = 0.00075\n",
      "Epoch 3675, loss = 0.00074\n",
      "Epoch 3700, loss = 0.00074\n",
      "Epoch 3725, loss = 0.00074\n",
      "Epoch 3750, loss = 0.00073\n",
      "Epoch 3775, loss = 0.00073\n",
      "Epoch 3800, loss = 0.00072\n",
      "Epoch 3825, loss = 0.00072\n",
      "Epoch 3850, loss = 0.00072\n",
      "Epoch 3875, loss = 0.00071\n",
      "Epoch 3900, loss = 0.00071\n",
      "Epoch 3925, loss = 0.00071\n",
      "Epoch 3950, loss = 0.00070\n",
      "Epoch 3975, loss = 0.00070\n",
      "Epoch 4000, loss = 0.00070\n",
      "Epoch 4025, loss = 0.00069\n",
      "Epoch 4050, loss = 0.00069\n",
      "Epoch 4075, loss = 0.00069\n",
      "Epoch 4100, loss = 0.00068\n",
      "Epoch 4125, loss = 0.00068\n",
      "Epoch 4150, loss = 0.00068\n",
      "Epoch 4175, loss = 0.00067\n",
      "Epoch 4200, loss = 0.00067\n",
      "Epoch 4225, loss = 0.00067\n",
      "Epoch 4250, loss = 0.00066\n",
      "Epoch 4275, loss = 0.00066\n",
      "Epoch 4300, loss = 0.00066\n",
      "Epoch 4325, loss = 0.00066\n",
      "Epoch 4350, loss = 0.00065\n",
      "Epoch 4375, loss = 0.00065\n",
      "Epoch 4400, loss = 0.00065\n",
      "Epoch 4425, loss = 0.00064\n",
      "Epoch 4450, loss = 0.00064\n",
      "Epoch 4475, loss = 0.00064\n",
      "Epoch 4500, loss = 0.00064\n",
      "Epoch 4525, loss = 0.00063\n",
      "Epoch 4550, loss = 0.00063\n",
      "Epoch 4575, loss = 0.00063\n",
      "Epoch 4600, loss = 0.00062\n",
      "Epoch 4625, loss = 0.00062\n",
      "Epoch 4650, loss = 0.00062\n",
      "Epoch 4675, loss = 0.00062\n",
      "Epoch 4700, loss = 0.00061\n",
      "Epoch 4725, loss = 0.00061\n",
      "Epoch 4750, loss = 0.00061\n",
      "Epoch 4775, loss = 0.00061\n",
      "Epoch 4800, loss = 0.00060\n",
      "Epoch 4825, loss = 0.00060\n",
      "Epoch 4850, loss = 0.00060\n",
      "Epoch 4875, loss = 0.00060\n",
      "Epoch 4900, loss = 0.00059\n",
      "Epoch 4925, loss = 0.00059\n",
      "Epoch 4950, loss = 0.00059\n",
      "Epoch 4975, loss = 0.00059\n",
      "mean accuracy score: 0.5847082494969819\n",
      "\n",
      "Node clustering evaluation has started...\n",
      "\n",
      "Epoch    0, loss = 1.03026\n",
      "Epoch   25, loss = 0.34859\n",
      "Epoch   50, loss = 0.11875\n",
      "Epoch   75, loss = 0.05549\n",
      "Epoch  100, loss = 0.03457\n",
      "Epoch  125, loss = 0.02537\n",
      "Epoch  150, loss = 0.02032\n",
      "Epoch  175, loss = 0.01708\n",
      "Epoch  200, loss = 0.01476\n",
      "Epoch  225, loss = 0.01296\n",
      "Epoch  250, loss = 0.01149\n",
      "Epoch  275, loss = 0.01025\n",
      "Epoch  300, loss = 0.00920\n",
      "Epoch  325, loss = 0.00830\n",
      "Epoch  350, loss = 0.00752\n",
      "Epoch  375, loss = 0.00686\n",
      "Epoch  400, loss = 0.00628\n",
      "Epoch  425, loss = 0.00579\n",
      "Epoch  450, loss = 0.00535\n",
      "Epoch  475, loss = 0.00498\n",
      "Epoch  500, loss = 0.00465\n",
      "Epoch  525, loss = 0.00436\n",
      "Epoch  550, loss = 0.00411\n",
      "Epoch  575, loss = 0.00388\n",
      "Epoch  600, loss = 0.00368\n",
      "Epoch  625, loss = 0.00350\n",
      "Epoch  650, loss = 0.00334\n",
      "Epoch  675, loss = 0.00319\n",
      "Epoch  700, loss = 0.00306\n",
      "Epoch  725, loss = 0.00294\n",
      "Epoch  750, loss = 0.00283\n",
      "Epoch  775, loss = 0.00273\n",
      "Epoch  800, loss = 0.00264\n",
      "Epoch  825, loss = 0.00255\n",
      "Epoch  850, loss = 0.00247\n",
      "Epoch  875, loss = 0.00240\n",
      "Epoch  900, loss = 0.00233\n",
      "Epoch  925, loss = 0.00227\n",
      "Epoch  950, loss = 0.00221\n",
      "Epoch  975, loss = 0.00215\n",
      "Epoch 1000, loss = 0.00210\n",
      "Epoch 1025, loss = 0.00205\n",
      "Epoch 1050, loss = 0.00200\n",
      "Epoch 1075, loss = 0.00196\n",
      "Epoch 1100, loss = 0.00192\n",
      "Epoch 1125, loss = 0.00188\n",
      "Epoch 1150, loss = 0.00184\n",
      "Epoch 1175, loss = 0.00180\n",
      "Epoch 1200, loss = 0.00177\n",
      "Epoch 1225, loss = 0.00173\n",
      "Epoch 1250, loss = 0.00170\n",
      "Epoch 1275, loss = 0.00167\n",
      "Epoch 1300, loss = 0.00165\n",
      "Epoch 1325, loss = 0.00162\n",
      "Epoch 1350, loss = 0.00159\n",
      "Epoch 1375, loss = 0.00157\n",
      "Epoch 1400, loss = 0.00154\n",
      "Epoch 1425, loss = 0.00152\n",
      "Epoch 1450, loss = 0.00150\n",
      "Epoch 1475, loss = 0.00148\n",
      "Epoch 1500, loss = 0.00146\n",
      "Epoch 1525, loss = 0.00144\n",
      "Epoch 1550, loss = 0.00142\n",
      "Epoch 1575, loss = 0.00140\n",
      "Epoch 1600, loss = 0.00138\n",
      "Epoch 1625, loss = 0.00136\n",
      "Epoch 1650, loss = 0.00135\n",
      "Epoch 1675, loss = 0.00133\n",
      "Epoch 1700, loss = 0.00132\n",
      "Epoch 1725, loss = 0.00130\n",
      "Epoch 1750, loss = 0.00129\n",
      "Epoch 1775, loss = 0.00127\n",
      "Epoch 1800, loss = 0.00126\n",
      "Epoch 1825, loss = 0.00124\n",
      "Epoch 1850, loss = 0.00123\n",
      "Epoch 1875, loss = 0.00122\n",
      "Epoch 1900, loss = 0.00121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1925, loss = 0.00120\n",
      "Epoch 1950, loss = 0.00118\n",
      "Epoch 1975, loss = 0.00117\n",
      "Epoch 2000, loss = 0.00116\n",
      "Epoch 2025, loss = 0.00115\n",
      "Epoch 2050, loss = 0.00114\n",
      "Epoch 2075, loss = 0.00113\n",
      "Epoch 2100, loss = 0.00112\n",
      "Epoch 2125, loss = 0.00111\n",
      "Epoch 2150, loss = 0.00110\n",
      "Epoch 2175, loss = 0.00109\n",
      "Epoch 2200, loss = 0.00108\n",
      "Epoch 2225, loss = 0.00107\n",
      "Epoch 2250, loss = 0.00106\n",
      "Epoch 2275, loss = 0.00106\n",
      "Epoch 2300, loss = 0.00105\n",
      "Epoch 2325, loss = 0.00104\n",
      "Epoch 2350, loss = 0.00103\n",
      "Epoch 2375, loss = 0.00102\n",
      "Epoch 2400, loss = 0.00102\n",
      "Epoch 2425, loss = 0.00101\n",
      "Epoch 2450, loss = 0.00100\n",
      "Epoch 2475, loss = 0.00099\n",
      "Epoch 2500, loss = 0.00099\n",
      "Epoch 2525, loss = 0.00098\n",
      "Epoch 2550, loss = 0.00097\n",
      "Epoch 2575, loss = 0.00097\n",
      "Epoch 2600, loss = 0.00096\n",
      "Epoch 2625, loss = 0.00095\n",
      "Epoch 2650, loss = 0.00095\n",
      "Epoch 2675, loss = 0.00094\n",
      "Epoch 2700, loss = 0.00093\n",
      "Epoch 2725, loss = 0.00093\n",
      "Epoch 2750, loss = 0.00092\n",
      "Epoch 2775, loss = 0.00092\n",
      "Epoch 2800, loss = 0.00091\n",
      "Epoch 2825, loss = 0.00090\n",
      "Epoch 2850, loss = 0.00090\n",
      "Epoch 2875, loss = 0.00089\n",
      "Epoch 2900, loss = 0.00089\n",
      "Epoch 2925, loss = 0.00088\n",
      "Epoch 2950, loss = 0.00088\n",
      "Epoch 2975, loss = 0.00087\n",
      "Epoch 3000, loss = 0.00087\n",
      "Epoch 3025, loss = 0.00086\n",
      "Epoch 3050, loss = 0.00085\n",
      "Epoch 3075, loss = 0.00085\n",
      "Epoch 3100, loss = 0.00084\n",
      "Epoch 3125, loss = 0.00084\n",
      "Epoch 3150, loss = 0.00083\n",
      "Epoch 3175, loss = 0.00083\n",
      "Epoch 3200, loss = 0.00083\n",
      "Epoch 3225, loss = 0.00082\n",
      "Epoch 3250, loss = 0.00082\n",
      "Epoch 3275, loss = 0.00081\n",
      "Epoch 3300, loss = 0.00081\n",
      "Epoch 3325, loss = 0.00080\n",
      "Epoch 3350, loss = 0.00080\n",
      "Epoch 3375, loss = 0.00079\n",
      "Epoch 3400, loss = 0.00079\n",
      "Epoch 3425, loss = 0.00078\n",
      "Epoch 3450, loss = 0.00078\n",
      "Epoch 3475, loss = 0.00078\n",
      "Epoch 3500, loss = 0.00077\n",
      "Epoch 3525, loss = 0.00077\n",
      "Epoch 3550, loss = 0.00076\n",
      "Epoch 3575, loss = 0.00076\n",
      "Epoch 3600, loss = 0.00076\n",
      "Epoch 3625, loss = 0.00075\n",
      "Epoch 3650, loss = 0.00075\n",
      "Epoch 3675, loss = 0.00074\n",
      "Epoch 3700, loss = 0.00074\n",
      "Epoch 3725, loss = 0.00074\n",
      "Epoch 3750, loss = 0.00073\n",
      "Epoch 3775, loss = 0.00073\n",
      "Epoch 3800, loss = 0.00072\n",
      "Epoch 3825, loss = 0.00072\n",
      "Epoch 3850, loss = 0.00072\n",
      "Epoch 3875, loss = 0.00071\n",
      "Epoch 3900, loss = 0.00071\n",
      "Epoch 3925, loss = 0.00071\n",
      "Epoch 3950, loss = 0.00070\n",
      "Epoch 3975, loss = 0.00070\n",
      "Epoch 4000, loss = 0.00070\n",
      "Epoch 4025, loss = 0.00069\n",
      "Epoch 4050, loss = 0.00069\n",
      "Epoch 4075, loss = 0.00069\n",
      "Epoch 4100, loss = 0.00068\n",
      "Epoch 4125, loss = 0.00068\n",
      "Epoch 4150, loss = 0.00068\n",
      "Epoch 4175, loss = 0.00067\n",
      "Epoch 4200, loss = 0.00067\n",
      "Epoch 4225, loss = 0.00067\n",
      "Epoch 4250, loss = 0.00066\n",
      "Epoch 4275, loss = 0.00066\n",
      "Epoch 4300, loss = 0.00066\n",
      "Epoch 4325, loss = 0.00066\n",
      "Epoch 4350, loss = 0.00065\n",
      "Epoch 4375, loss = 0.00065\n",
      "Epoch 4400, loss = 0.00065\n",
      "Epoch 4425, loss = 0.00064\n",
      "Epoch 4450, loss = 0.00064\n",
      "Epoch 4475, loss = 0.00064\n",
      "Epoch 4500, loss = 0.00064\n",
      "Epoch 4525, loss = 0.00063\n",
      "Epoch 4550, loss = 0.00063\n",
      "Epoch 4575, loss = 0.00063\n",
      "Epoch 4600, loss = 0.00062\n",
      "Epoch 4625, loss = 0.00062\n",
      "Epoch 4650, loss = 0.00062\n",
      "Epoch 4675, loss = 0.00062\n",
      "Epoch 4700, loss = 0.00061\n",
      "Epoch 4725, loss = 0.00061\n",
      "Epoch 4750, loss = 0.00061\n",
      "Epoch 4775, loss = 0.00061\n",
      "Epoch 4800, loss = 0.00060\n",
      "Epoch 4825, loss = 0.00060\n",
      "Epoch 4850, loss = 0.00060\n",
      "Epoch 4875, loss = 0.00060\n",
      "Epoch 4900, loss = 0.00060\n",
      "Epoch 4925, loss = 0.00059\n",
      "Epoch 4950, loss = 0.00059\n",
      "Epoch 4975, loss = 0.00059\n",
      "0.1371392182352232\n",
      "0.15674363310489467\n",
      "0.1479895421131114\n",
      "0.14869676645737295\n",
      "0.1241543527046519\n",
      "0.159237222889518\n",
      "0.142736530710374\n",
      "0.15715353632953968\n",
      "0.1388632010960542\n",
      "0.15006213104755453\n",
      "Model evaluation took: 155.71976947784424 seconds\n",
      "Bernoulli_gaussian_gaussian_64_False\n",
      "\n",
      "Link prediction evaluation has started...\n",
      "\n",
      "\n",
      "Round: 1\n",
      "\n",
      "Epoch    0, loss = 0.01479\n",
      "\n",
      "Round: 2\n",
      "\n",
      "Epoch    0, loss = 0.01484\n",
      "\n",
      "Round: 3\n",
      "\n",
      "Epoch    0, loss = 0.01479\n",
      "\n",
      "Round: 4\n",
      "\n",
      "Epoch    0, loss = 0.01488\n",
      "\n",
      "Round: 5\n",
      "\n",
      "Epoch    0, loss = 0.01472\n",
      "\n",
      "Round: 6\n",
      "\n",
      "Epoch    0, loss = 0.01486\n",
      "\n",
      "Round: 7\n",
      "\n",
      "Epoch    0, loss = 0.01480\n",
      "\n",
      "Round: 8\n",
      "\n",
      "Epoch    0, loss = 0.01481\n",
      "\n",
      "Round: 9\n",
      "\n",
      "Epoch    0, loss = 0.01474\n",
      "\n",
      "Round: 10\n",
      "\n",
      "Epoch    0, loss = 0.01484\n",
      "\n",
      "=> mean auc score: 0.5708689782881864\n",
      "\n",
      "Node classification evaluation has started...\n",
      "\n",
      "Epoch    0, loss = 0.01854\n",
      "Epoch   25, loss = 0.01504\n",
      "Epoch   50, loss = 0.01242\n",
      "Epoch   75, loss = 0.01079\n",
      "Epoch  100, loss = 0.00952\n",
      "Epoch  125, loss = 0.00848\n",
      "Epoch  150, loss = 0.00758\n",
      "Epoch  175, loss = 0.00679\n",
      "Epoch  200, loss = 0.00613\n",
      "Epoch  225, loss = 0.00564\n",
      "Epoch  250, loss = 0.00530\n",
      "Epoch  275, loss = 0.00509\n",
      "Epoch  300, loss = 0.00495\n",
      "Epoch  325, loss = 0.00486\n",
      "Epoch  350, loss = 0.00479\n",
      "Epoch  375, loss = 0.00475\n",
      "Epoch  400, loss = 0.00471\n",
      "Epoch  425, loss = 0.00468\n",
      "Epoch  450, loss = 0.00466\n",
      "Epoch  475, loss = 0.00465\n",
      "Epoch  500, loss = 0.00463\n",
      "Epoch  525, loss = 0.00462\n",
      "Epoch  550, loss = 0.00461\n",
      "Epoch  575, loss = 0.00461\n",
      "Epoch  600, loss = 0.00460\n",
      "Epoch  625, loss = 0.00459\n",
      "Epoch  650, loss = 0.00459\n",
      "Epoch  675, loss = 0.00459\n",
      "Epoch  700, loss = 0.00458\n",
      "Epoch  725, loss = 0.00458\n",
      "Epoch  750, loss = 0.00458\n",
      "Epoch  775, loss = 0.00457\n",
      "Epoch  800, loss = 0.00457\n",
      "Epoch  825, loss = 0.00457\n",
      "Epoch  850, loss = 0.00457\n",
      "Epoch  875, loss = 0.00457\n",
      "Epoch  900, loss = 0.00457\n",
      "Epoch  925, loss = 0.00456\n",
      "Epoch  950, loss = 0.00456\n",
      "Epoch  975, loss = 0.00456\n",
      "Epoch 1000, loss = 0.00456\n",
      "Epoch 1025, loss = 0.00456\n",
      "Epoch 1050, loss = 0.00456\n",
      "Epoch 1075, loss = 0.00456\n",
      "Epoch 1100, loss = 0.00456\n",
      "Epoch 1125, loss = 0.00456\n",
      "Epoch 1150, loss = 0.00456\n",
      "Epoch 1175, loss = 0.00456\n",
      "Epoch 1200, loss = 0.00456\n",
      "Epoch 1225, loss = 0.00456\n",
      "Epoch 1250, loss = 0.00455\n",
      "Epoch 1275, loss = 0.00455\n",
      "Epoch 1300, loss = 0.00455\n",
      "Epoch 1325, loss = 0.00455\n",
      "Epoch 1350, loss = 0.00455\n",
      "Epoch 1375, loss = 0.00455\n",
      "Epoch 1400, loss = 0.00455\n",
      "Epoch 1425, loss = 0.00455\n",
      "Epoch 1450, loss = 0.00455\n",
      "Epoch 1475, loss = 0.00455\n",
      "Epoch 1500, loss = 0.00455\n",
      "Epoch 1525, loss = 0.00455\n",
      "Epoch 1550, loss = 0.00455\n",
      "Epoch 1575, loss = 0.00455\n",
      "Epoch 1600, loss = 0.00455\n",
      "Epoch 1625, loss = 0.00455\n",
      "Epoch 1650, loss = 0.00455\n",
      "Epoch 1675, loss = 0.00455\n",
      "Epoch 1700, loss = 0.00455\n",
      "Epoch 1725, loss = 0.00455\n",
      "Epoch 1750, loss = 0.00455\n",
      "Epoch 1775, loss = 0.00455\n",
      "Epoch 1800, loss = 0.00455\n",
      "Epoch 1825, loss = 0.00455\n",
      "Epoch 1850, loss = 0.00455\n",
      "Epoch 1875, loss = 0.00455\n",
      "Epoch 1900, loss = 0.00455\n",
      "Epoch 1925, loss = 0.00455\n",
      "Epoch 1950, loss = 0.00455\n",
      "Epoch 1975, loss = 0.00455\n",
      "Epoch 2000, loss = 0.00455\n",
      "Epoch 2025, loss = 0.00455\n",
      "Epoch 2050, loss = 0.00455\n",
      "Epoch 2075, loss = 0.00455\n",
      "Epoch 2100, loss = 0.00455\n",
      "Epoch 2125, loss = 0.00455\n",
      "Epoch 2150, loss = 0.00455\n",
      "Epoch 2175, loss = 0.00455\n",
      "Epoch 2200, loss = 0.00455\n",
      "Epoch 2225, loss = 0.00455\n",
      "Epoch 2250, loss = 0.00455\n",
      "Epoch 2275, loss = 0.00455\n",
      "Epoch 2300, loss = 0.00455\n",
      "Epoch 2325, loss = 0.00455\n",
      "Epoch 2350, loss = 0.00455\n",
      "Epoch 2375, loss = 0.00455\n",
      "Epoch 2400, loss = 0.00455\n",
      "Epoch 2425, loss = 0.00455\n",
      "Epoch 2450, loss = 0.00455\n",
      "Epoch 2475, loss = 0.00455\n",
      "Epoch 2500, loss = 0.00455\n",
      "Epoch 2525, loss = 0.00455\n",
      "Epoch 2550, loss = 0.00455\n",
      "Epoch 2575, loss = 0.00455\n",
      "Epoch 2600, loss = 0.00455\n",
      "Epoch 2625, loss = 0.00455\n",
      "Epoch 2650, loss = 0.00455\n",
      "Epoch 2675, loss = 0.00455\n",
      "Epoch 2700, loss = 0.00455\n",
      "Epoch 2725, loss = 0.00455\n",
      "Epoch 2750, loss = 0.00455\n",
      "Epoch 2775, loss = 0.00455\n",
      "Epoch 2800, loss = 0.00455\n",
      "Epoch 2825, loss = 0.00455\n",
      "Epoch 2850, loss = 0.00455\n",
      "Epoch 2875, loss = 0.00455\n",
      "Epoch 2900, loss = 0.00455\n",
      "Epoch 2925, loss = 0.00455\n",
      "Epoch 2950, loss = 0.00455\n",
      "Epoch 2975, loss = 0.00455\n",
      "Epoch 3000, loss = 0.00455\n",
      "Epoch 3025, loss = 0.00455\n",
      "Epoch 3050, loss = 0.00455\n",
      "Epoch 3075, loss = 0.00455\n",
      "Epoch 3100, loss = 0.00455\n",
      "Epoch 3125, loss = 0.00455\n",
      "Epoch 3150, loss = 0.00455\n",
      "Epoch 3175, loss = 0.00455\n",
      "Epoch 3200, loss = 0.00455\n",
      "Epoch 3225, loss = 0.00455\n",
      "Epoch 3250, loss = 0.00455\n",
      "Epoch 3275, loss = 0.00455\n",
      "Epoch 3300, loss = 0.00455\n",
      "Epoch 3325, loss = 0.00455\n",
      "Epoch 3350, loss = 0.00455\n",
      "Epoch 3375, loss = 0.00455\n",
      "Epoch 3400, loss = 0.00455\n",
      "Epoch 3425, loss = 0.00455\n",
      "Epoch 3450, loss = 0.00455\n",
      "Epoch 3475, loss = 0.00455\n",
      "Epoch 3500, loss = 0.00455\n",
      "Epoch 3525, loss = 0.00455\n",
      "Epoch 3550, loss = 0.00455\n",
      "Epoch 3575, loss = 0.00455\n",
      "Epoch 3600, loss = 0.00455\n",
      "Epoch 3625, loss = 0.00455\n",
      "Epoch 3650, loss = 0.00455\n",
      "Epoch 3675, loss = 0.00455\n",
      "Epoch 3700, loss = 0.00455\n",
      "Epoch 3725, loss = 0.00455\n",
      "Epoch 3750, loss = 0.00455\n",
      "Epoch 3775, loss = 0.00455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3800, loss = 0.00455\n",
      "Epoch 3825, loss = 0.00455\n",
      "Epoch 3850, loss = 0.00455\n",
      "Epoch 3875, loss = 0.00455\n",
      "Epoch 3900, loss = 0.00455\n",
      "Epoch 3925, loss = 0.00455\n",
      "Epoch 3950, loss = 0.00455\n",
      "Epoch 3975, loss = 0.00455\n",
      "Epoch 4000, loss = 0.00455\n",
      "Epoch 4025, loss = 0.00455\n",
      "Epoch 4050, loss = 0.00455\n",
      "Epoch 4075, loss = 0.00455\n",
      "Epoch 4100, loss = 0.00455\n",
      "Epoch 4125, loss = 0.00455\n",
      "Epoch 4150, loss = 0.00455\n",
      "Epoch 4175, loss = 0.00455\n",
      "Epoch 4200, loss = 0.00455\n",
      "Epoch 4225, loss = 0.00455\n",
      "Epoch 4250, loss = 0.00455\n",
      "Epoch 4275, loss = 0.00455\n",
      "Epoch 4300, loss = 0.00455\n",
      "Epoch 4325, loss = 0.00455\n",
      "Epoch 4350, loss = 0.00455\n",
      "Epoch 4375, loss = 0.00455\n",
      "Epoch 4400, loss = 0.00455\n",
      "Epoch 4425, loss = 0.00455\n",
      "Epoch 4450, loss = 0.00455\n",
      "Epoch 4475, loss = 0.00455\n",
      "Epoch 4500, loss = 0.00455\n",
      "Epoch 4525, loss = 0.00455\n",
      "Epoch 4550, loss = 0.00455\n",
      "Epoch 4575, loss = 0.00455\n",
      "Epoch 4600, loss = 0.00455\n",
      "Epoch 4625, loss = 0.00455\n",
      "Epoch 4650, loss = 0.00454\n",
      "Epoch 4675, loss = 0.00454\n",
      "Epoch 4700, loss = 0.00454\n",
      "Epoch 4725, loss = 0.00454\n",
      "Epoch 4750, loss = 0.00454\n",
      "Epoch 4775, loss = 0.00454\n",
      "Epoch 4800, loss = 0.00454\n",
      "Epoch 4825, loss = 0.00454\n",
      "Epoch 4850, loss = 0.00454\n",
      "Epoch 4875, loss = 0.00454\n",
      "Epoch 4900, loss = 0.00454\n",
      "Epoch 4925, loss = 0.00454\n",
      "Epoch 4950, loss = 0.00454\n",
      "Epoch 4975, loss = 0.00454\n",
      "mean accuracy score: 0.8454728370221328\n",
      "\n",
      "Node clustering evaluation has started...\n",
      "\n",
      "Epoch    0, loss = 0.01849\n",
      "Epoch   25, loss = 0.01499\n",
      "Epoch   50, loss = 0.01238\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2411b861a0d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m             NMI = evaluate_node_clustering.exp_Node_Clustering(A,y,dataset,model,exp[\"node_clustering_num_rounds\"],\n\u001b[1;32m    143\u001b[0m                                            \u001b[0mnode_clustering_folder\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                                            eval_epochs=exp[\"node_clustering_eval_epochs\"],undirected=True)\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mtotal_NMI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNMI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/students/winter-term-2019/project_5/cziakas/new_project/project-5/evaluation/evaluate_node_clustering.py\u001b[0m in \u001b[0;36mexp_Node_Clustering\u001b[0;34m(AdjMat, Y, dataset_name, embedding_method, rounds, result_folder, train_epochs, eval_epochs, undirected)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{dataset_name} & {embedding_method.get_method_summary()}: \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mset_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdjMat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mnorm_MI_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mround_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/students/winter-term-2019/project_5/cziakas/new_project/project-5/evaluation/evaluate_node_clustering.py\u001b[0m in \u001b[0;36mcompute_embedding\u001b[0;34m(embedding_method, AdjMat, eval_epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0membedding_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0membedding_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_model_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdjMat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/students/winter-term-2019/project_5/cziakas/new_project/project-5/embedding/bernoulli.py\u001b[0m in \u001b[0;36mlearn_embedding\u001b[0;34m(self, num_epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch_begin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/students/winter-term-2019/project_5/cziakas/new_project/project-5/embedding/bernoulli.py\u001b[0m in \u001b[0;36mcompute_loss_gaussian\u001b[0;34m(adj, emb, b)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mpdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mneg_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mneg_term\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mpos_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mpdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mneg_term\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experiment 1\n",
    "\n",
    "exp = {\n",
    "    \"max_epochs\": 1000,\n",
    "    \"learning_rate\": 1e-2, #Adam\n",
    "    \"weight_decay\": 1e-7,\n",
    "    \n",
    "    \"link_prediction\":True,\n",
    "    \"link_pred_num_rounds\": 10,\n",
    "    \"link_pred_train_ratio\": 0.8,\n",
    "    \"link_pred_eval_every_n_steps\": 20,\n",
    "    \"link_pred_edge_emb_method\": \"average\",\n",
    "    \n",
    "    \"node_classification\": True,\n",
    "    \"node_class_num_rounds\": 10,\n",
    "    \"node_class_train_ratio\": 0.8,\n",
    "    \"node_class_eval_every_n_steps\": 50,\n",
    "    \n",
    "    \"node_clustering\": True,\n",
    "    \"node_clustering_num_rounds\": 10,\n",
    "    \"node_clustering_eval_epochs\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "datasets = [\"cora\"]#, \"citeseer\", \"polblogs\",\"hvr\"] \n",
    "\n",
    "#TO DO:\n",
    "#Laplacian (nobrainer) : Till done\n",
    "#NetMF : Tillcd\n",
    "#PPR : Till done(?)\n",
    "#Sum_Power_Tran : Jan\n",
    "#Sim_Rank : Jan\n",
    "#Big Task: what to do with W / U Epsilon : Jan\n",
    "#possible: Forest Fire / Stochastic Optimization\n",
    "\n",
    "\n",
    "#Bernoulli\n",
    "model_01 = Bernoulli(embedding_dimension=64, decoder='sigmoid') #works!\n",
    "model_02 = Bernoulli(embedding_dimension=64, decoder='sigmoid', W_enabled=True)\n",
    "model_03 = Bernoulli(embedding_dimension=64, decoder='gaussian') #works!\n",
    "model_04 = Bernoulli(embedding_dimension=64, decoder='exponential') #works!\n",
    "#model_05 = Bernoulli, exponential with W\n",
    "\n",
    "\n",
    "#KL\n",
    "#KL(similarity_measure{needs to be row stochastic}|softmax(ZZ^T))\n",
    "model_06 = KL(embedding_dimension=64, decoder='softmax', similarity_measure='ppr')\n",
    "model_07 = KL(embedding_dimension=64, decoder='softmax', similarity_measure='sum_power_tran')\n",
    "model_08 = KL(embedding_dimension=64, decoder='softmax', similarity_measure='sim_rank')\n",
    "model_09 = KL(embedding_dimension=64, decoder='softmax', similarity_measure='transition')\n",
    "model_10 = KL(embedding_dimension=64, decoder='softmax', similarity_measure='ppr', W_enabled=True)\n",
    "model_11 = KL(embedding_dimension=64, decoder='softmax', similarity_measure='sum_power_tran', W_enabled=True)\n",
    "model_12 = KL(embedding_dimension=64, decoder='softmax', similarity_measure='sim_rank', W_enabled=True)\n",
    "model_13 = KL(embedding_dimension=64, decoder='softmax', similarity_measure='transition', W_enabled=True)\n",
    "\n",
    "\n",
    "#Matrix Factorization\n",
    "model_14 = MatrixFactorization(embedding_dimension=64, similarity_measure='adjacency', embedding_option=1) #works!\n",
    "model_15 = MatrixFactorization(embedding_dimension=64, similarity_measure='laplacian', embedding_option=1) \n",
    "model_16 = MatrixFactorization(embedding_dimension=64, similarity_measure='transition', embedding_option=1)\n",
    "model_17 = MatrixFactorization(embedding_dimension=64, similarity_measure='sym_normalized_laplacian', embedding_option=1) #works!\n",
    "model_18 = MatrixFactorization(embedding_dimension=64, similarity_measure='NetMF', embedding_option=1)\n",
    "model_19 = MatrixFactorization(embedding_dimension=64, similarity_measure='ppr', embedding_option=1)\n",
    "model_20 = MatrixFactorization(embedding_dimension=64, similarity_measure='sum_power_tran', embedding_option=1)\n",
    "model_21 = MatrixFactorization(embedding_dimension=64, similarity_measure='sim_rank', embedding_option=1)\n",
    "\n",
    "model_22 = MatrixFactorization(embedding_dimension=64, similarity_measure='adjacency', embedding_option=2) #works!\n",
    "model_23 = MatrixFactorization(embedding_dimension=64, similarity_measure='laplacian', embedding_option=2) \n",
    "model_24 = MatrixFactorization(embedding_dimension=64, similarity_measure='transition', embedding_option=2)\n",
    "model_25 = MatrixFactorization(embedding_dimension=64, similarity_measure='sym_normalized_laplacian', embedding_option=2) #works!\n",
    "model_26 = MatrixFactorization(embedding_dimension=64, similarity_measure='NetMF', embedding_option=2)\n",
    "model_27 = MatrixFactorization(embedding_dimension=64, similarity_measure='ppr', embedding_option=2)\n",
    "model_28 = MatrixFactorization(embedding_dimension=64, similarity_measure='sum_power_tran', embedding_option=2)\n",
    "model_29 = MatrixFactorization(embedding_dimension=64, similarity_measure='sim_rank', embedding_option=2)\n",
    "\n",
    "# model_14, model_15, model_16, \n",
    "embedding_methods = [\n",
    "            model_01, model_03,\n",
    "            model_06,model_07, model_09,\n",
    "            model_14,model_15,model_16,model_17,\n",
    "            model_18,model_19,model_20 ]\n",
    "\n",
    "\n",
    "dset_NMI=[]\n",
    "dset_AUC=[]\n",
    "dset_ACC=[]\n",
    "# setup folders to store experiment setup summary and results\n",
    "result_folder = plot_util.setup_folders_and_summary_files(exp, datasets, embedding_methods)\n",
    "print(f'The results of the current experiment are stored at experiments/{result_folder}')\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    # load dataset\n",
    "    total_NMI=[]\n",
    "    total_AUC=[]\n",
    "    total_ACC=[]\n",
    "    A, y = graph_util.load_dataset(dataset)\n",
    "    \n",
    "    for model in embedding_methods:\n",
    "        \n",
    "        print(model.get_method_summary())\n",
    "        start = time.time()\n",
    "        \n",
    "        directory= 'results_plot/'+dataset+'/'+str(model._method_name)\n",
    "        if(not os.path.isdir(directory)):\n",
    "            os.mkdir(directory)\n",
    "        \n",
    "        # do link prediction\n",
    "        if(exp[\"link_prediction\"]):\n",
    "            link_prediction_folder = result_folder + \"/link_prediction\"\n",
    "            AUC= evaluate_link_prediction.expLP(A,dataset,model,exp[\"link_pred_num_rounds\"],\n",
    "                                           link_prediction_folder, train_ratio=exp[\"link_pred_train_ratio\"], \n",
    "                                           edge_emb_method=exp[\"link_pred_edge_emb_method\"],train_epochs=exp[\"max_epochs\"],\n",
    "                                           eval_epochs=exp[\"link_pred_eval_every_n_steps\"], undirected=True)\n",
    "            total_AUC.append(AUC)\n",
    "            \n",
    "            if(not os.path.isdir(directory+'/link_prediction')):\n",
    "                os.mkdir(directory+'/link_prediction')\n",
    "                \n",
    "            with open(directory+'/link_prediction/'+str(model._similarity_measure)+'_AUC_results.txt','w') as f:\n",
    "                for listitem in AUC:\n",
    "                    f.write('%s\\n' % listitem) \n",
    "                    \n",
    "        # do node classification\n",
    "        if(exp[\"node_classification\"]):\n",
    "            node_classification_folder = result_folder + \"/node_classification\"\n",
    "            ACC = evaluate_node_classification.expNC(A,y,dataset,model,exp[\"node_class_num_rounds\"],\n",
    "                                               node_classification_folder, train_ratio=exp[\"node_class_train_ratio\"],\n",
    "                                               train_epochs=exp[\"max_epochs\"],eval_epochs=exp[\"node_class_eval_every_n_steps\"],undirected=True)\n",
    "            total_ACC.append(ACC)\n",
    "            \n",
    "            if(not os.path.isdir(directory+'/node_classification')):\n",
    "                os.mkdir(directory+'/node_classification')\n",
    "                \n",
    "            with open(directory+'/node_classification/'+str(model._similarity_measure)+'_ACC_results.txt','w') as f:\n",
    "                for listitem in ACC:\n",
    "                    f.write('%s\\n' % listitem) \n",
    "         \n",
    "        # do node clustering\n",
    "        if(exp[\"node_clustering\"]):\n",
    "            node_clustering_folder = result_folder + \"/node_clustering\"\n",
    "            NMI = evaluate_node_clustering.exp_Node_Clustering(A,y,dataset,model,exp[\"node_clustering_num_rounds\"],\n",
    "                                           node_clustering_folder , train_epochs=exp[\"max_epochs\"],\n",
    "                                           eval_epochs=exp[\"node_clustering_eval_epochs\"],undirected=True)\n",
    "            total_NMI.append(NMI)\n",
    "            \n",
    "            if(not os.path.isdir(directory+'/node_clustering')):\n",
    "                os.mkdir(directory+'/node_clustering')\n",
    "                \n",
    "            with open(directory+'/node_clustering/'+str(model._similarity_measure)+'_NMI_results.txt','w') as f:\n",
    "                for listitem in NMI:\n",
    "                    f.write('%s\\n' % listitem)    \n",
    "                    \n",
    "        end = time.time()\n",
    "        print(f'Model evaluation took: {end-start} seconds')\n",
    "        \n",
    "    dset_NMI.append(total_NMI)\n",
    "    dset_AUC.append(total_AUC)\n",
    "    dset_ACC.append(total_ACC)\n",
    "    \n",
    "    \n",
    "headers= [  'sig','gau',\n",
    "            'ppr','spt','sr',\n",
    "            'adj','lap','ftr','nl','nf', 'fppr', 'fspt'\n",
    "         ]\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('results_plot/cora/cora_dset_NMI.txt', 'wb') as f:\n",
    "    pickle.dump(dset_NMI, f)\n",
    "    \n",
    "with open('results_plot/cora/cora_dset_AUC.txt', 'wb') as f:\n",
    "    pickle.dump(dset_AUC, f)\n",
    "\n",
    "with open('results_plot/cora/cora_dset_ACC.txt', 'wb') as f:\n",
    "    pickle.dump(dset_ACC, f)\n",
    "\n",
    "for ds,NMI in zip(datasets,dset_NMI):\n",
    "    plt.figure()\n",
    "    sns_plot = sns.boxplot(x=headers, y=NMI);\n",
    "    plt.title(\"Comparison for node clustering on \"+ds)\n",
    "    plt.xlabel(\"model\")\n",
    "    plt.ylabel(\"NMI\")\n",
    "    plt.ylim([0.,1.])\n",
    "    sns_plot.figure.savefig('plots/'+ds+'/NMI_'+ds+'.png')\n",
    "    \n",
    "for ds,AUC in zip(datasets,dset_AUC):\n",
    "    plt.figure()\n",
    "    sns_plot = sns.boxplot(x=headers, y=AUC);\n",
    "    plt.title(\"Comparison for link prediction on \"+ds)\n",
    "    plt.xlabel(\"model\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.ylim([0.,1.])\n",
    "    sns_plot.figure.savefig('plots/'+ds+'/AUC_'+ds+'.png')\n",
    "    \n",
    "for ds,ACC in zip(datasets,dset_ACC):\n",
    "    plt.figure()\n",
    "    sns_plot = sns.boxplot(x=headers, y=ACC);\n",
    "    plt.title(\"Comparison for link prediction on \"+ds)\n",
    "    plt.xlabel(\"model\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.ylim([0.,1.])\n",
    "    sns_plot.figure.savefig('plots/'+ds+'/ACC_'+ds+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "with open('results_plot/cora/cora_dset_NMI.txt', 'wb') as f:\n",
    "    pickle.dump(dset_NMI, f)\n",
    "    \n",
    "with open('results_plot/cora/cora_dset_AUC.txt', 'wb') as f:\n",
    "    pickle.dump(dset_AUC, f)\n",
    "\n",
    "with open('results_plot/cora/cora_dset_ACC.txt', 'wb') as f:\n",
    "    pickle.dump(dset_ACC, f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#with open('a.txt', 'rb') as f:\n",
    "#   my_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "headers= [  'sig','gau',\n",
    "            'ppr','spt','sr',\n",
    "            'adj','lap','ftr','nl','nf', 'fppr', 'fspt'\n",
    "         ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ds,NMI in zip(datasets,dset_NMI):\n",
    "    plt.figure()\n",
    "    sns_plot = sns.boxplot(x=headers, y=NMI);\n",
    "    plt.title(\"Comparison for node clustering on \"+ds)\n",
    "    plt.xlabel(\"model\")\n",
    "    plt.ylabel(\"NMI\")\n",
    "    plt.ylim([0.,1.])\n",
    "    sns_plot.figure.savefig('plots/'+ds+'/NMI_'+ds+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds,AUC in zip(datasets,dset_AUC):\n",
    "    plt.figure()\n",
    "    sns_plot = sns.boxplot(x=headers, y=AUC);\n",
    "    plt.title(\"Comparison for link prediction on \"+ds)\n",
    "    plt.xlabel(\"model\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.ylim([0.,1.])\n",
    "    sns_plot.figure.savefig('plots/'+ds+'/AUC_'+ds+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds,ACC in zip(datasets,dset_ACC):\n",
    "    plt.figure()\n",
    "    sns_plot = sns.boxplot(x=headers, y=ACC);\n",
    "    plt.title(\"Comparison for link prediction on \"+ds)\n",
    "    plt.xlabel(\"model\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.ylim([0.,1.])\n",
    "    sns_plot.figure.savefig('plots/'+ds+'/ACC_'+ds+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
